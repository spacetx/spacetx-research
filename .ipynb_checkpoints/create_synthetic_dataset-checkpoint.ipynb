{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SYNTHETIC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch -U\n",
    "#!pip3 install torchvision -U\n",
    "#!pip3 install pyro-ppl==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyro'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0f2a62505278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#import numpy as np\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyro'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro\n",
    "import torch \n",
    "#import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from LOW_LEVEL_UTILITIES.utilities import show_batch, save_obj, load_obj\n",
    "from LOW_LEVEL_UTILITIES.utilities import DatasetInMemory\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "print(\"pyro.__version__  --> \",pyro.__version__)\n",
    "print(\"torch.__version__ --> \",torch.__version__)\n",
    "assert(pyro.__version__.startswith('0.4.0'))\n",
    "assert(torch.__version__.startswith('1.2.0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"N_MIN_OBJECTS\": 2,\n",
    "    \"N_MAX_OBJECTS\": 6,\n",
    "    \"MIN_OBJECT_SIZE\": 20,\n",
    "    \"MAX_OBJECT_SIZE\": 35,\n",
    "    \"WIDTH_RAW_IMAGE\": 80,\n",
    "    \"HEIGHT_RAW_IMAGE\": 80,\n",
    "    \"BACK_GROUND_MEAN\":0.4,\n",
    "    \"BACK_GROUND_STD\":0.2,\n",
    "    \"OBJECT_MAX_INTENSITY\": 0.8,\n",
    "    \"OVERLAP_THRESHOLD\" : 0.2,\n",
    "    \"Z_BG_DIM\" : 20,\n",
    "    \"BG_IS_SALT_AND_PEPPER\" : False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BG_generator(torch.nn.Module):\n",
    "    \"\"\" Use transpose convolution to create a long distance pattern \n",
    "        z_bg =  torch.randn(8*params['Z_BG_DIM']).view(8,-1)\n",
    "        bg_mu,bg_std = bg_gen(z_bg)\n",
    "        bg = bg_mu+bg_std*torch.randn(bg_mu.shape)\n",
    "        show_batch(bg)\n",
    "    \"\"\"\n",
    "    def __init__(self, params: dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_z_bg = params[\"Z_BG_DIM\"]\n",
    "        self.width    = params[\"WIDTH_RAW_IMAGE\"]\n",
    "        self.height   = params[\"HEIGHT_RAW_IMAGE\"]\n",
    "        \n",
    "        self.upsample = torch.nn.Linear(self.dim_z_bg, 6 * 7 * 7)\n",
    "        \n",
    "        # Parameters UNet\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(6, 1, kernel_size=4, stride=2, padding=1), # DOUBLE THE SIZE, i.e. 14x14\n",
    "            torch.nn.Upsample(size=(self.width,self.height), scale_factor=None, mode='bilinear', align_corners=True)\n",
    "        )\n",
    "            \n",
    "    def forward(self,z):\n",
    "        \n",
    "        batch_size,dim_z = z.shape\n",
    "        x0 = self.upsample(z).view(batch_size,6,7,7)\n",
    "        x_mu = torch.sigmoid(self.decoder(x0))\n",
    "        return xmu\n",
    "    \n",
    "def make_single_disks(disk_size=28,n_disk=400):\n",
    "    x_mat = torch.linspace(-1.0, 1.0,steps=disk_size).view(-1,1).expand(disk_size,disk_size).float()\n",
    "    y_mat = x_mat.clone().permute(1,0)\n",
    "    r2   = (x_mat**2 + y_mat**2)\n",
    "    disk = (r2<1.0).float()\n",
    "    fake_labels = disk.new_zeros(n_disk)\n",
    "    return disk[None,None,...].expand(n_disk,-1,-1,-1),fake_labels\n",
    "\n",
    "\n",
    "def compute_affine_aligned(bx_dimfull,by_dimfull,bw_dimfull,bh_dimfull,width_raw_image,height_raw_image):\n",
    "        \"\"\" The affine matrix desxcribes the mapping between the source and the target \n",
    "            This particular affine matrix encodes only scaling and translation\n",
    "\n",
    "            | x_s |   | sx  0   kx | | x_t |   | sx  0  | | x_t |   | kx |\n",
    "            |     | = |            | | y_t | = | 0   sy | | y_t | + | ky |\n",
    "            | y_s |   | 0   sy  ky | | 1   |     \n",
    "\n",
    "            From bx,by,bw,bh dimfull we can get:\n",
    "            sx =  width/bw\n",
    "            sy = height/bh\n",
    "            kx = (width-2*bx)/bw\n",
    "            ky = (height-2*by)/bh\n",
    "        \"\"\" \n",
    "        \n",
    "        # Preparation the variable of interest  \n",
    "        zero = (torch.zeros_like(bx_dimfull)).view(-1)\n",
    "        sx = (width_raw_image/bw_dimfull).view(-1)\n",
    "        sy = (height_raw_image/bh_dimfull).view(-1)\n",
    "        kx = ((width_raw_image-2*bx_dimfull)/bw_dimfull).view(-1)\n",
    "        ky = ((height_raw_image-2*by_dimfull)/bh_dimfull).view(-1)\n",
    "        \n",
    "        # Affine matrix\n",
    "        affine = torch.stack((zero,sx,sy,kx,ky), dim=1)\n",
    "        indeces_resampling = torch.LongTensor([1, 0, 3, 0, 2, 4]).to(affine.device) # indeces to sample: sx,0,kx,0,sy,ky\n",
    "        return torch.index_select(affine, 1, indeces_resampling).view(-1,2,3) \n",
    "   \n",
    "\n",
    "def create_dataset(params, dataset_single_objects, bg_generator, N_imgs=100):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset_single_objects, batch_size=params[\"N_MAX_OBJECTS\"], shuffle=True)\n",
    "    \n",
    "    N_max_objects = params[\"N_MAX_OBJECTS\"]\n",
    "    width  = params[\"WIDTH_RAW_IMAGE\"]\n",
    "    height = params[\"HEIGHT_RAW_IMAGE\"]\n",
    "    delta_size = params[\"MAX_OBJECT_SIZE\"]-params[\"MIN_OBJECT_SIZE\"]\n",
    "    delta_n_digits = params[\"N_MAX_OBJECTS\"]-params[\"N_MIN_OBJECTS\"]\n",
    "    threshold_overlap = params[\"OVERLAP_THRESHOLD\"] \n",
    "    z_bg_dim = params['Z_BG_DIM']\n",
    "    \n",
    "    many_objects = torch.zeros(N_max_objects,1,width,height)\n",
    "    \n",
    "    imgs  = torch.zeros(N_imgs,1,width,height)\n",
    "    count = torch.zeros(N_imgs)\n",
    "    clean_imgs = torch.zeros_like(imgs)\n",
    "\n",
    "    for l in range(N_imgs):\n",
    "        if(l%100 == 0):\n",
    "            print(l,\" out of \",N_imgs)\n",
    "        \n",
    "        # Draw the random variables\n",
    "        bw_dimfull = params[\"MIN_OBJECT_SIZE\"] + delta_size*torch.rand(N_max_objects)\n",
    "        bh_dimfull = bw_dimfull\n",
    "        bx_dimfull = torch.rand(N_max_objects)*(width-bw_dimfull)+0.5*bw_dimfull\n",
    "        by_dimfull = torch.rand(N_max_objects)*(height-bh_dimfull)+0.5*bh_dimfull\n",
    "               \n",
    "        # Compute the affine matrix\n",
    "        affine_matrix = compute_affine_aligned(bx_dimfull,by_dimfull,bw_dimfull,bh_dimfull,width,height)\n",
    "        assert(affine_matrix.shape == (N_max_objects,2,3))\n",
    "    \n",
    "        # Sample individual object\n",
    "        single_object, labels = next(iter(dataloader))\n",
    "        squares = torch.ones_like(single_object)\n",
    "        grid = F.affine_grid(affine_matrix, many_objects.shape) \n",
    "        many_objects  = F.grid_sample(single_object, grid, mode='bilinear', padding_mode='zeros') \n",
    "        many_squares = (F.grid_sample(squares, grid, mode='bilinear', padding_mode='zeros') >0).float() \n",
    "        #do this so that many_suqare is a binary variable. This is NOT the case after bilinear interpolation\n",
    "\n",
    "        \n",
    "        # Compute the Intersection_Over_Union of the squares\n",
    "        batch_size = many_squares.shape[0]\n",
    "        flatten = many_squares.view(batch_size,-1)\n",
    "        area = torch.sum(flatten,dim=-1)\n",
    "        min_area = torch.min(area.unsqueeze(0),area.unsqueeze(1))\n",
    "        intersec = torch.sum(flatten.unsqueeze(0)*flatten.unsqueeze(1),dim=-1)\n",
    "        IoMIN = intersec/min_area\n",
    "        cluster_mask = (IoMIN > threshold_overlap).float()\n",
    "        \n",
    "        # Select the non-overlapping squares\n",
    "        impossible  = torch.zeros(batch_size)\n",
    "        chosen      = torch.zeros(batch_size)\n",
    "        while 1 < 2:\n",
    "            possible = 1 - impossible\n",
    "            index = torch.argmax(possible) \n",
    "            chosen[index] = 1.0\n",
    "            impossible = torch.clamp(torch.matmul(cluster_mask,chosen),max=1)\n",
    "            if( (impossible == 1).all() ):\n",
    "                break\n",
    "        \n",
    "        # Count the number of non-overlapping objects\n",
    "        # and select a random subset of them\n",
    "        cumsum = torch.cumsum(chosen,dim=0)\n",
    "        n_chosen = cumsum[-1]\n",
    "        indicator = chosen\n",
    "        \n",
    "        # FOREGROUND obtained by summing the non-overlapping objects\n",
    "        fg = params[\"OBJECT_MAX_INTENSITY\"]*torch.clamp(torch.sum(indicator[:,None,None,None]*many_objects,dim=0),max=1.0)\n",
    "        bg_mask = (fg <0.1).float()\n",
    "        #print(\"bg_mask.shape\",bg_mask.shape)\n",
    "        \n",
    "        # Make a structured background\n",
    "        if(params[\"BG_IS_SALT_AND_PEPPER\"]):\n",
    "            bg_scaled = params[\"BACK_GROUND_MEAN\"]+params[\"BACK_GROUND_STD\"]*torch.randn(bg_mask.shape)\n",
    "        else:\n",
    "            z_bg = torch.randn(z_bg_dim).view(1,-1)\n",
    "            bg = bg_generator(z_bg)\n",
    "            #bg = bg_mu+0.1*torch.randn(bg_mu.shape)\n",
    "            bg_scaled = params[\"BACK_GROUND_MEAN\"]+params[\"BACK_GROUND_STD\"]*(bg - bg.mean())/bg.std()\n",
    "        \n",
    "        #print(\"bg_scaled.shape\",bg_scaled.shape)\n",
    "        \n",
    "        # Add the background and clamp to 1.0\n",
    "        imgs[l,...] = torch.clamp(fg+bg_scaled*bg_mask,min=0.0,max=1.0)\n",
    "        count[l] = n_chosen\n",
    "        clean_imgs[l,...]=fg\n",
    "        \n",
    "    return imgs,count,clean_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single MNIST dataset and Single DISK dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_dir_single_MNIST = \"/home/ldalessi/DATA/SINGLE_MNIST/\"\n",
    "#write_dir_single_FashionMNIST = \"/home/ldalessi/DATA/SINGLE_FashionMNIST/\"\n",
    "\n",
    "#write_dir_single_FashionMNIST = \"/home/jupyter/REPOS/DATA/SINGLE_FashionMNIST/\"\n",
    "#write_dir_single_MNIST = \"/home/jupyter/REPOS/DATA/SINGLE_MNIST/\"\n",
    "\n",
    "write_dir_single_MNIST = \"/Users/ldalessi/DAPI_unsupervised/DATA/SINGLE_MNIST/\"\n",
    "write_dir_single_FashionMNIST = \"/Users/ldalessi/DAPI_unsupervised/DATA/SINGLE_FashionMNIST/\"\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "mnist_trainset = torchvision.datasets.MNIST(write_dir_single_MNIST, train=True, download=True, transform=transform)\n",
    "mnist_testset  = torchvision.datasets.MNIST(write_dir_single_MNIST, train=False, download=True, transform=transform)\n",
    "\n",
    "Fashionmnist_trainset = torchvision.datasets.FashionMNIST(write_dir_single_FashionMNIST, train=True, download=True, transform=transform)\n",
    "Fashionmnist_testset  = torchvision.datasets.FashionMNIST(write_dir_single_FashionMNIST, train=False, download=True, transform=transform)\n",
    "\n",
    "disks,labels = make_single_disks(disk_size=28,n_disk=400)\n",
    "disks_trainset = torch.utils.data.TensorDataset(disks,labels)\n",
    "disks_testset = torch.utils.data.TensorDataset(disks,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the MULTI OBJECT DATASET from SINGLE OBJECT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  out of  10000\n",
      "100  out of  10000\n",
      "200  out of  10000\n",
      "300  out of  10000\n",
      "400  out of  10000\n",
      "500  out of  10000\n",
      "600  out of  10000\n",
      "700  out of  10000\n",
      "800  out of  10000\n",
      "900  out of  10000\n",
      "1000  out of  10000\n",
      "1100  out of  10000\n",
      "1200  out of  10000\n",
      "1300  out of  10000\n",
      "1400  out of  10000\n",
      "1500  out of  10000\n",
      "1600  out of  10000\n",
      "1700  out of  10000\n",
      "1800  out of  10000\n",
      "1900  out of  10000\n",
      "2000  out of  10000\n",
      "2100  out of  10000\n",
      "2200  out of  10000\n",
      "2300  out of  10000\n",
      "2400  out of  10000\n",
      "2500  out of  10000\n",
      "2600  out of  10000\n",
      "2700  out of  10000\n",
      "2800  out of  10000\n",
      "2900  out of  10000\n",
      "3000  out of  10000\n",
      "3100  out of  10000\n",
      "3200  out of  10000\n",
      "3300  out of  10000\n",
      "3400  out of  10000\n",
      "3500  out of  10000\n",
      "3600  out of  10000\n",
      "3700  out of  10000\n",
      "3800  out of  10000\n",
      "3900  out of  10000\n",
      "4000  out of  10000\n",
      "4100  out of  10000\n",
      "4200  out of  10000\n",
      "4300  out of  10000\n",
      "4400  out of  10000\n",
      "4500  out of  10000\n",
      "4600  out of  10000\n",
      "4700  out of  10000\n",
      "4800  out of  10000\n",
      "4900  out of  10000\n",
      "5000  out of  10000\n",
      "5100  out of  10000\n",
      "5200  out of  10000\n",
      "5300  out of  10000\n",
      "5400  out of  10000\n",
      "5500  out of  10000\n",
      "5600  out of  10000\n",
      "5700  out of  10000\n",
      "5800  out of  10000\n",
      "5900  out of  10000\n",
      "6000  out of  10000\n",
      "6100  out of  10000\n",
      "6200  out of  10000\n",
      "6300  out of  10000\n",
      "6400  out of  10000\n",
      "6500  out of  10000\n",
      "6600  out of  10000\n",
      "6700  out of  10000\n",
      "6800  out of  10000\n",
      "6900  out of  10000\n",
      "7000  out of  10000\n",
      "7100  out of  10000\n",
      "7200  out of  10000\n",
      "7300  out of  10000\n",
      "7400  out of  10000\n",
      "7500  out of  10000\n",
      "7600  out of  10000\n",
      "7700  out of  10000\n",
      "7800  out of  10000\n",
      "7900  out of  10000\n",
      "8000  out of  10000\n",
      "8100  out of  10000\n",
      "8200  out of  10000\n",
      "8300  out of  10000\n",
      "8400  out of  10000\n",
      "8500  out of  10000\n",
      "8600  out of  10000\n",
      "8700  out of  10000\n",
      "8800  out of  10000\n",
      "8900  out of  10000\n",
      "9000  out of  10000\n",
      "9100  out of  10000\n",
      "9200  out of  10000\n",
      "9300  out of  10000\n",
      "9400  out of  10000\n",
      "9500  out of  10000\n",
      "9600  out of  10000\n",
      "9700  out of  10000\n",
      "9800  out of  10000\n",
      "9900  out of  10000\n"
     ]
    }
   ],
   "source": [
    "bg_generator = BG_generator(params)\n",
    "\n",
    "#multi_mnist_train = create_dataset(params,mnist_trainset,bg_generator,N_imgs=60000)  \n",
    "#multi_mnist_test = create_dataset(params,mnist_testset,bg_generator,N_imgs=1000) \n",
    "\n",
    "#multi_mnist_train = create_dataset(params,mnist_trainset,bg_generator,N_imgs=10)  \n",
    "#multi_mnist_test = create_dataset(params,mnist_testset,bg_generator,N_imgs=10) \n",
    "\n",
    "#multi_Fashionmnist_train = create_dataset(params,Fashionmnist_trainset,bg_generator,N_imgs=60)  \n",
    "#multi_Fashionmnist_test = create_dataset(params,Fashionmnist_testset,bg_generator,N_imgs=10) \n",
    "#multi_Fashionmnist_train = create_dataset(params,Fashionmnist_trainset,bg_generator,N_imgs=60000)  \n",
    "#multi_Fashionmnist_test = create_dataset(params,Fashionmnist_testset,bg_generator,N_imgs=1000) \n",
    "\n",
    "\n",
    "multi_Fashionmnist_train = create_dataset(params,Fashionmnist_trainset,bg_generator,N_imgs=100)  \n",
    "multi_Fashionmnist_test = create_dataset(params,Fashionmnist_testset,bg_generator,N_imgs=10) \n",
    "\n",
    "#multi_Fashionmnist_train = create_dataset(params,Fashionmnist_trainset,bg_generator,N_imgs=10000)  \n",
    "#multi_Fashionmnist_test = create_dataset(params,Fashionmnist_testset,bg_generator,N_imgs=1000) \n",
    "\n",
    "#multi_disk_train = create_dataset(params,disks_trainset,bg_generator,N_imgs=10)  \n",
    "#multi_disk_test = create_dataset(params,disks_testset,bg_generator,N_imgs=10) \n",
    "\n",
    "#multi_disk_train = create_dataset(params,disks_trainset,bg_generator,N_imgs=2000)  \n",
    "#multi_disk_test = create_dataset(params,disks_testset,bg_generator,N_imgs=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgs,labels = multi_disk_test\n",
    "#imgs,labels = multi_mnist_test\n",
    "#imgs,labels,clean_imgs = multi_Fashionmnist_test\n",
    "#imgs,labels,clean_imgs = multi_Fashionmnist_train\n",
    "#print(\"min and max number of ojjects\",torch.min(labels),torch.max(labels))\n",
    "#print(\"imgs.shape\",imgs.shape)\n",
    "#print(\"labels\",labels[:8])\n",
    "#show_batch(imgs[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset on file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_dir_multiple_MNIST = \"/home/ldalessi/MULTI_MNIST/\"\n",
    "#write_dir_multiple_MNIST = \"/home/ldalessi/DATA/MULTI_MNIST/\"\n",
    "#write_dir_multiple_FashionMNIST = \"/home/ldalessi/DATA/MULTI_FashionMNIST/\"\n",
    "write_dir_multiple_FashionMNIST = \"/Users/ldalessi/DAPI_unsupervised/DATA/MULTI_FashionMNIST/\"\n",
    "\n",
    "save_obj(multi_Fashionmnist_train[:2],write_dir_multiple_FashionMNIST,\"multi_Fashionmnist_train_small_structured_noise\")\n",
    "save_obj(multi_Fashionmnist_test[:2],write_dir_multiple_FashionMNIST,\"multi_Fashionmnist_test_small_structured_noise\")\n",
    "\n",
    "#save_obj(multi_Fashionmnist_train[:2],write_dir_multiple_FashionMNIST,\"multi_Fashionmnist_train_large_structured_noise\")\n",
    "#save_obj(multi_Fashionmnist_test[:2],write_dir_multiple_FashionMNIST,\"multi_Fashionmnist_test_large_structured_noise\")\n",
    "\n",
    "#save_obj(multi_mnist_train[:2],write_dir_multiple_MNIST,\"multi_mnist_train_large\")\n",
    "#save_obj(multi_mnist_test[:2],write_dir_multiple_MNIST,\"multi_mnist_test_large\")\n",
    "\n",
    "#write_dir_multiple_DISK = \"/home/ldalessi/MULTI_MDISK/\"\n",
    "#write_dir_multiple_DISK = \"/home/ldalessi/DATA/MULTI_DISK/\"\n",
    "\n",
    "#save_obj(multi_disk_train[:2],write_dir_multiple_DISK,\"multi_disk_train_with_bg\")\n",
    "#save_obj(multi_disk_test[:2],write_dir_multiple_DISK,\"multi_disk_test_with_bg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
