{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compositional VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT NECESSARY MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python_version() --->  3.7.7\n",
      "torch.__version__ -->  1.5.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" #for debugging, it decrease performance dramatically\n",
    "\n",
    "# Check versions\n",
    "from platform import python_version\n",
    "print(\"python_version() ---> \",python_version())\n",
    "print(\"torch.__version__ --> \",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML, Image\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from MODULES.utilities import *\n",
    "from MODULES.vae_model import *\n",
    "from MODULES.namedtuple import PreProcess, ImageBbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We expect to find the file \"parameters.json\" in the execution directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wdl.file_train': 'streamlined_4_smFISH_stitched_OLEH.tif',\n",
       " 'wdl.file_test': 'dummy_1.pkl',\n",
       " 'wdl.file_ckpt': 'dummy_2.pkl',\n",
       " 'wdl.dir_output': 'merfish_res4_aug_27_v2',\n",
       " 'simulation': {'__comment': 'there are 3 types of runs: scratch, resume, pretrained',\n",
       "  'type': 'scratch',\n",
       "  'MAX_EPOCHS': 10000,\n",
       "  'TEST_FREQUENCY': 100,\n",
       "  'CHECKPOINT_FREQUENCY': 100,\n",
       "  'batch_size': 64},\n",
       " 'architecture': {'__comment': 'architecture parameters, level_zwhere_output is between 0 and n_max_pool included',\n",
       "  'dim_zinstance': 20,\n",
       "  'dim_zwhere': 4,\n",
       "  'dim_logit': 1,\n",
       "  'cropped_size': 28,\n",
       "  'n_max_pool': 5,\n",
       "  'level_zwhere_and_logit_output': 3,\n",
       "  'level_background_output': 5,\n",
       "  'n_ch_output_features': 32,\n",
       "  'n_ch_after_first_two_conv': 32},\n",
       " 'input_image': {'__comment': 'parameters describing the input images',\n",
       "  'n_objects_max': 25,\n",
       "  'size_object_min': 10,\n",
       "  'size_object_max': 40,\n",
       "  'length_scale_GP': 8.0,\n",
       "  'size_raw_image': 160,\n",
       "  'ch_in': 1,\n",
       "  'bg_is_zero': True,\n",
       "  'background_resolution_before_upsampling': [2, 2]},\n",
       " 'nms': {'__comment': 'parameters for the non-max-suppression',\n",
       "  'overlap_threshold': 0.3},\n",
       " 'GECO_loss': {'__comment': 'if active=false use ELBO, else use GECO with Log-Likelihood threshold = n_pixels * n_channel * threshold',\n",
       "  'is_active': True,\n",
       "  'factor_balance_range': [0.1, 0.7, 0.8],\n",
       "  'factor_sparsity_range': [-1, 1.0, 100],\n",
       "  'target_fg_fraction': [0.1, 0.25],\n",
       "  'target_mse': [0.75, 1.0],\n",
       "  'bg_std': 0.03,\n",
       "  'fg_std': 0.03},\n",
       " 'optimizer': {'__comment': 'which optimizer to use',\n",
       "  'type': 'adam',\n",
       "  'base_lr': 0.001,\n",
       "  'betas': [0.9, 0.999],\n",
       "  'base_lr_geco': 0.001,\n",
       "  'betas_geco': [0.9, 0.999],\n",
       "  'weight_decay': 0.0,\n",
       "  'eps': 1e-08,\n",
       "  'scheduler_is_active': True,\n",
       "  'scheduler_type': 'step_LR',\n",
       "  'scheduler_step_size': 1000,\n",
       "  'scheduler_gamma': 0.75},\n",
       " 'shortcut_prob_corr_factor': {'__comment': 'parameters for the shortcut for porb_corr_factor',\n",
       "  'values': [0.5, 0.0],\n",
       "  'times': [200, 500]},\n",
       " 'soft_constraint': {'__comment': 'all the parameters about the soft constraints',\n",
       "  'overlap': {'__comment': 'cost which discourages masks from overlapping',\n",
       "   'strength': 0.01,\n",
       "   'exponent': 1},\n",
       "  'mask_volume_absolute': {'__comment': 'cost which discourage masks which are too large or too small',\n",
       "   'lower_bound_value': 40,\n",
       "   'lower_bound_width': 5,\n",
       "   'lower_bound_strength': 0,\n",
       "   'lower_bound_exponent': 2,\n",
       "   'upper_bound_value': 300,\n",
       "   'upper_bound_width': 5,\n",
       "   'upper_bound_strength': 0,\n",
       "   'upper_bound_exponent': 2}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = load_json_as_dict(\"parameters_MERFISH.json\")\n",
    "#params = load_json_as_dict(\"parameters_VISIUM.json\")\n",
    "#params = load_json_as_dict(\"parameters.json\")\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare the file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "streamlined_4_smFISH_stitched_OLEH.tif\n",
      "dummy_1.pkl\n",
      "dummy_2.pkl\n",
      "NEW_ARCHIVE/merfish_res4_aug_27_v2\n",
      "NEW_ARCHIVE/merfish_res4_aug_27_v2/parameters.json\n",
      "NEW_ARCHIVE/merfish_res4_aug_27_v2/tiling.pt\n"
     ]
    }
   ],
   "source": [
    "# CROMWELL will localize: \n",
    "# gs://ld-data-bucket/data/fashionmnist_train.pkl -> execution_dir/ld-data-bucket/data/fashionmnist_train.pkl\n",
    "# Therefore I just need to remove  \"gs://\"\n",
    "# Note that every path is relative to the execution_dir\n",
    "\n",
    "train_file = params[\"wdl.file_train\"]\n",
    "test_file = params[\"wdl.file_test\"]\n",
    "ckpt_file = params[\"wdl.file_ckpt\"]\n",
    "dir_output = params[\"wdl.dir_output\"]\n",
    "\n",
    "dir_output = os.path.join(\"NEW_ARCHIVE\", dir_output)\n",
    "tiling_file = os.path.join(dir_output, \"tiling.pt\")\n",
    "\n",
    "# create output directionry if it does nto exists\n",
    "try:\n",
    "    os.mkdir(dir_output)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# Save input_json_file to output dir\n",
    "json_param_file = os.path.join(dir_output, \"parameters.json\")\n",
    "save_dict_as_json(params,json_param_file)\n",
    "\n",
    "# checks\n",
    "assert os.path.isfile(train_file)\n",
    "assert os.path.isfile(test_file)\n",
    "assert os.path.isfile(ckpt_file)\n",
    "    \n",
    "print(train_file)\n",
    "print(test_file)\n",
    "print(ckpt_file)\n",
    "print(dir_output)\n",
    "print(json_param_file)\n",
    "print(tiling_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"get the data\")\n",
    "#train_file=\"multi_disk_train_shading_bg_v3.pkl\"\n",
    "#test_file=\"multi_disk_test_shading_bg_v3.pkl\"\n",
    "#multi_mnist_test_no_bg.pkl\n",
    "\n",
    "#train_file=\"multi_mnist_train_simple_v3.pkl\"\n",
    "#test_file=\"multi_mnist_test_simple_v3.pkl\"\n",
    "\n",
    "##train_file=\"multi_disk_train_shading_bg.pkl\"\n",
    "##test_file=\"multi_disk_test_shading_bg.pkl\"\n",
    "\n",
    "#x_train, y_train = load_obj(train_file)\n",
    "#train_loader = LoaderInMemory(x=x_train, \n",
    "#                              y=y_train, \n",
    "#                              pin_in_cuda_memory=torch.cuda.is_available(),\n",
    "#                              batch_size=params[\"simulation\"][\"batch_size\"],  \n",
    "#                              shuffle=True)\n",
    "#                               \n",
    "#x_test, y_test = load_obj(train_file)\n",
    "#test_loader = LoaderInMemory(x=x_test, \n",
    "#                             y=y_test, \n",
    "#                             pin_in_cuda_memory=torch.cuda.is_available(),\n",
    "#                             batch_size=params[\"simulation\"][\"batch_size\"],  \n",
    "#                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIELDS -> ('img', 'roi_mask', 'bbox_original', 'bbox_crop')\n",
      "DEBUG -> torch.Size([1, 1, 5376, 5376]) torch.float64\n"
     ]
    }
   ],
   "source": [
    "preprocessed = load_obj(train_file)\n",
    "print(\"FIELDS ->\", preprocessed._fields)\n",
    "\n",
    "my_shape = preprocessed.img.shape\n",
    "my_dtype = preprocessed.img.dtype\n",
    "                   \n",
    "print(\"DEBUG ->\",my_shape, my_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed images and convert to float torch in (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5376, 5376])\n",
      "torch.Size([1, 1, 5376, 5376])\n",
      "GPU GB -> 0.0\n"
     ]
    }
   ],
   "source": [
    "img_torch = preprocessed.img.float()\n",
    "roi_mask_torch = preprocessed.roi_mask.bool()\n",
    "assert len(img_torch.shape) == len(roi_mask_torch.shape) == 4\n",
    "print(img_torch.shape)\n",
    "print(roi_mask_torch.shape)\n",
    "print(\"GPU GB ->\",torch.cuda.memory_allocated()/1E9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 160, 160])\n",
      "GPU GB -> 0.0\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = params[\"simulation\"][\"batch_size\"]\n",
    "SIZE_CROPS = params[\"input_image\"][\"size_raw_image\"]\n",
    "\n",
    "N_test = 256\n",
    "N_train = 128\n",
    "conditional_crop_test = ConditionalRandomCrop(desired_w=SIZE_CROPS, desired_h=SIZE_CROPS, \n",
    "                                              min_roi_fraction=0.9, n_crops_per_image=N_test)\n",
    "\n",
    "conditional_crop_train = ConditionalRandomCrop(desired_w=SIZE_CROPS, desired_h=SIZE_CROPS, \n",
    "                                               min_roi_fraction=0.9, n_crops_per_image=N_train)\n",
    "\n",
    "test_data = conditional_crop_test.forward(img=img_torch,\n",
    "                                          roi_mask=roi_mask_torch)\n",
    "\n",
    "print(test_data.shape)\n",
    "print(\"GPU GB ->\",torch.cuda.memory_allocated()/1E9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU GB -> 0.0\n"
     ]
    }
   ],
   "source": [
    "test_loader = SpecialDataSet(img=test_data,\n",
    "                             store_in_cuda=False,\n",
    "                             shuffle=False,\n",
    "                             drop_last=False,\n",
    "                             batch_size=BATCH_SIZE)\n",
    "print(\"GPU GB ->\",torch.cuda.memory_allocated()/1E9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset lenght: 256\n",
      "img.shape torch.Size([256, 1, 160, 160])\n",
      "img.dtype torch.float32\n",
      "img.device cpu\n",
      "MINIBATCH: img.shapes labels.shape, index.shape -> torch.Size([8, 1, 160, 160]) torch.Size([8]) torch.Size([8])\n",
      "MINIBATCH: min and max of minibatch tensor(0.0301) tensor(0.7362)\n",
      "GPU GB -> 0.0\n"
     ]
    }
   ],
   "source": [
    "test_loader.check_batch()\n",
    "print(\"GPU GB ->\",torch.cuda.memory_allocated()/1E9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU GB -> 0.0\n"
     ]
    }
   ],
   "source": [
    "train_loader = SpecialDataSet(img=img_torch, \n",
    "                              roi_mask=roi_mask_torch,\n",
    "                              data_augmentation=conditional_crop_train,\n",
    "                              store_in_cuda=False,\n",
    "                              shuffle=True,\n",
    "                              drop_last=True,\n",
    "                              batch_size=BATCH_SIZE)\n",
    "print(\"GPU GB ->\",torch.cuda.memory_allocated()/1E9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset lenght: 128\n",
      "img.shape torch.Size([128, 1, 5376, 5376])\n",
      "img.dtype torch.float32\n",
      "img.device cpu\n",
      "MINIBATCH: img.shapes labels.shape, index.shape -> torch.Size([8, 1, 160, 160]) torch.Size([8]) torch.Size([8])\n",
      "MINIBATCH: min and max of minibatch tensor(0.0301) tensor(0.8605)\n",
      "GPU GB -> 0.0\n"
     ]
    }
   ],
   "source": [
    "train_loader.check_batch()\n",
    "print(\"GPU GB ->\",torch.cuda.memory_allocated()/1E9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU GB -> 0.0\n"
     ]
    }
   ],
   "source": [
    "params = load_json_as_dict(json_param_file)\n",
    "params\n",
    "print(\"GPU GB ->\",torch.cuda.memory_allocated()/1E9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU GB -> 0.127860224\n"
     ]
    }
   ],
   "source": [
    "vae = CompositionalVae(params)\n",
    "optimizer = instantiate_optimizer(model=vae, dict_params_optimizer=params[\"optimizer\"])\n",
    "print(\"GPU GB ->\",torch.cuda.memory_allocated()/1E9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the UNET resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0300]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.sigma_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(1., device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.geco_sparsity_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "imgs_in.shape, imgs_out.shape torch.Size([4, 1, 160, 160]) torch.Size([11, 4, 1, 160, 160])\n",
      "GPU GB -> 0.127860224\n"
     ]
    }
   ],
   "source": [
    "imgs_in, labels, index = test_loader.load(batch_size=4)\n",
    "imgs_out = vae.inference_and_generator.unet.show_grid(imgs_in)\n",
    "\n",
    "print(imgs_in.device)\n",
    "print(\"imgs_in.shape, imgs_out.shape\", imgs_in.shape, imgs_out.shape)\n",
    "\n",
    "show_batch(imgs_out[:,0])\n",
    "print(\"GPU GB ->\",torch.cuda.memory_allocated()/1E9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the vae architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does the untrained generator match the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS BLOCK MAKES MEMORY GO CRAZY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 160, 160])\n",
      "GPU GB -> 0.173494272\n"
     ]
    }
   ],
   "source": [
    "vae.eval()\n",
    "if torch.cuda.is_available():\n",
    "    imgs_in = imgs_in.cuda()\n",
    "generated_data = vae.generate(imgs_in=imgs_in, draw_boxes=True, draw_bg=False)\n",
    "tmp = torch.cat((imgs_in.expand(-1,3,-1,-1),generated_data.imgs[:4].expand(-1,3,-1,-1)), dim=0)\n",
    "print(tmp.shape)\n",
    "show_batch(tmp, title=\"data AND generated imgs\", figsize=(24,24))\n",
    "print(\"GPU GB ->\",torch.cuda.memory_allocated()/1E9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU GB -> 0.173494272\n",
      "GPU GB -> 0.127860224\n"
     ]
    }
   ],
   "source": [
    "show_batch(generated_data.inference.p_map, n_padding=2, title=\"generated probability\", figsize=(24,24))\n",
    "print(\"GPU GB ->\",torch.cuda.memory_allocated()/1E9)\n",
    "del generated_data\n",
    "del tmp\n",
    "del imgs_in\n",
    "print(\"GPU GB ->\",torch.cuda.memory_allocated()/1E9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__comment': 'all the parameters about the soft constraints',\n",
       " 'overlap': {'__comment': 'cost which discourages masks from overlapping',\n",
       "  'strength': 0.01,\n",
       "  'exponent': 1},\n",
       " 'mask_volume_absolute': {'__comment': 'cost which discourage masks which are too large or too small',\n",
       "  'lower_bound_value': 40,\n",
       "  'lower_bound_width': 5,\n",
       "  'lower_bound_strength': 0,\n",
       "  'lower_bound_exponent': 2,\n",
       "  'upper_bound_value': 300,\n",
       "  'upper_bound_width': 5,\n",
       "  'upper_bound_strength': 0,\n",
       "  'upper_bound_exponent': 2}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.dict_soft_constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd9f5e39190>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1=torch.linspace(0,1,100)\n",
    "y1=sample_from_constraints_dict(dict_soft_constraints=vae.dict_soft_constraints,\n",
    "                                var_name=\"overlap\", \n",
    "                                var_value=x1, \n",
    "                                verbose=False)\n",
    "plt.plot(x1,y1, label=\"overlap\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd9f5e08110>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWYUlEQVR4nO3df5BU5b3n8fcXBBHFmKDxIkN2RhZDQBTNSEwwidFggMuFrL+ClhHNGsqKVjC7lEKMi1rR8kZz449QUuQqUS8uargqWmy8/k6ZxB+DDr8c0JFw4wgb5pJa1ESDyLN/TMMdxp6hh56ZBp/3q6qr+zznOed8z0M3nz6nT/dESglJUr56VboASVJlGQSSlDmDQJIyZxBIUuYMAknK3H6VLmBPHHrooam6urrSZUjSPmXZsmX/kVI6rG37PhkE1dXV1NXVVboMSdqnRMS/F2v31JAkZc4gkKTMGQSSlLl98jMCaV/3wQcf0NTUxPvvv1/pUvQx1K9fP6qqqujTp09J/Q0CqQKampoYMGAA1dXVRESly9HHSEqJzZs309TURE1NTUnLeGpIqoD333+fgQMHGgLqchHBwIEDO3W0aRBIFWIIqLt09rllEEhS5gwCScqcQSCpS6xfv56jjz66W9b9y1/+kksvvbRb1r0nuqqek08+ebe/kvDQQw/x6quvlr2tjhgEkrQX64kg8PJRqcKueWQ1r254u0vXOeKIg5nzDyM77LN+/XrGjx/PSSedxPPPP8+xxx7LhRdeyJw5c9i0aRMLFy4E4LLLLuO9997jgAMOYMGCBXz2s59l9erVXHjhhWzdupXt27ezePHiXa5ZX7duHWeccQbz58/nhBNO+Mi2v/CFL3DnnXcycmRLjSeffDI//elPqamp4Tvf+Q7r1q2jf//+zJ8/n2OOOWaXZS+44AImTZrEmWeeCcBBBx3Eu+++yzPPPMOcOXM4/PDDqa+v5/TTT2fUqFHccsstvPfeezz00EMMHTqU5uZmLr74Yv74xz8CcPPNNzN27NiiY/Tiiy8W3X+AN998k/Hjx/OHP/yBc889lzlz5vCXv/yFs88+m6amJj788EOuuuoqvvWtb/Hkk08yc+ZMtm3bxgknnMDtt9/O/vvvv8u2duwHwK9+9SseffRRpk+fzpIlS3j22Wf58Y9/zOLFiwG45JJLaG5upn///vziF79g+PDhHf5b745HBFLGGhsbmTFjBitWrGDNmjXce++9PPfcc9x0001cf/31DB8+nN/85je88sorXHvttfzwhz8EYN68ecyYMYP6+nrq6uqoqqrauc61a9dyxhlnsGDBgqIhADB16lTuv/9+ADZu3MiGDRv4/Oc/z5w5czjuuONYsWIF119/Peeff36n9mf58uXccsstrFy5knvuuYfXXnuNF198kYsuuojbbrsNgBkzZvCDH/yAl156icWLF3PRRRe1u7729h9aQmLhwoXU19fzwAMPUFdXx69//WuOOOIIli9fzqpVqxg/fjzvv/8+F1xwAffddx8rV65k27Zt3H777SXtz5e+9CUmT57MjTfeSH19PUOHDmX69OncdtttLFu2jJtuuonvfe97nRqjYjwikCpsd+/cu1NNTQ2jRo0CYOTIkZx66qlEBKNGjWL9+vVs2bKFadOm8frrrxMRfPDBBwB88Ytf5LrrrqOpqYnTTz+dYcOGAdDc3MyUKVNYvHjxznf7xZx99tmMGzeOa665hvvvv5+zzjoLgOeee27nu95TTjmFzZs3s2XLlpL354QTTmDQoEEADB06lNNOOw2AUaNG8fTTTwPwxBNP7HKq5e233+add95hwIABH1lfe/sPMG7cOAYOHAjA6aefznPPPcfEiROZOXMmV1xxBZMmTeLLX/4yy5cvp6amhqOOOgqAadOmMXfuXC677LKS92uHd999l9/97nc7xwvgb3/7W6fX05ZHBFLGWp+e6NWr187pXr16sW3bNq666iq+9rWvsWrVKh555JGdX1I699xzWbJkCQcccADf+MY3eOqppwD4xCc+wZAhQ/jtb3/b4XYHDx7MwIEDWbFiBffddx9Tp04FWr4V21bba+L3228/tm/fvrP/1q1bS94fgO3bt/P73/+e+vp66uvreeutt4qGANDu/herKyI46qijWLZsGaNGjWL27Nlce+21RfepmNbra+/LYNu3b+eQQw7ZWXt9fT0NDQ0lrb8jBoGkdm3ZsoXBgwcDLVfK7LBu3TqOPPJIvv/97zN58mRWrFgBQN++fXnooYe4++67uffeeztc99SpU/nJT37Cli1bdh6VfOUrX9n52cQzzzzDoYceysEHH7zLctXV1SxbtgyAhx9+eJd36aU47bTT+PnPf75zur6+vt2+7e0/wOOPP86f//znnZ8/jB07lg0bNtC/f3/OO+88Zs6cycsvv8zw4cNZv349jY2NANxzzz189atf/ci2Dj/8cBoaGti+fTsPPvjgzvYBAwbwzjvvAHDwwQdTU1PDAw88ALQE4fLlyzu1/8UYBJLadfnllzN79mzGjh3Lhx9+uLP9vvvu4+ijj2b06NGsWbNml3P5Bx54II8++ig/+9nPePjhh9td95lnnsmiRYs4++yzd7ZdffXV1NXVccwxxzBr1izuuuuujyz33e9+l2effZYxY8bwwgsvcOCBB3Zqn2699dad2xgxYgTz5s3r9P4DnHTSSXz7299m9OjRnHHGGdTW1rJy5UrGjBnD6NGjue666/jRj35Ev379WLBgAWeddRajRo2iV69eXHzxxR/Z1g033MCkSZM45ZRTdp7egpbAvPHGGznuuON44403WLhwIXfccQfHHnssI0eO7HCMSxWlHrbsTWpra5N/oUz7soaGBj73uc9Vugx9jBV7jkXEspRSbdu+HhFIUua8akhSt3nssce44oordmmrqanZ5Rz43mDBggXccsstu7SNHTuWuXPnVqiinuWpIakCGhoaGD58uL9Aqm6RUmLNmjWeGpL2Zv369WPz5s0lX1oolWrHH6bp169fyct4akiqgKqqKpqammhubq50KfoY2vGnKktlEEgV0KdPn5L/jKDU3Tw1JEmZ65IgiIjxEbE2IhojYlaR+RERtxbmr4iI49vM7x0Rr0TEo11RjySpdGUHQUT0BuYCE4ARwDkRMaJNtwnAsMJtOtD2p/dmAOX/YIYkqdO64ohgDNCYUlqXUtoKLAKmtOkzBbg7tXgeOCQiBgFERBXw98A/d0EtkqRO6oogGAy82Wq6qdBWap+bgcuB7R1tJCKmR0RdRNR5pYUkdZ2uCIJi34hpe3F00T4RMQnYlFJatruNpJTmp5RqU0q1hx122J7UKUkqoiuCoAkY0mq6CthQYp+xwOSIWE/LKaVTIuJfuqAmSVKJuiIIXgKGRURNRPQFpgJL2vRZApxfuHroRGBLSmljSml2SqkqpVRdWO6plNJ5XVCTJKlEZX+hLKW0LSIuBR4DegN3ppRWR8TFhfnzgKXARKAR+CtwYbnblSR1DX90TpIy4Y/OSZKKMggkKXMGgSRlziCQpMwZBJKUOYNAkjJnEEhS5gwCScqcQSBJmTMIJClzBoEkZc4gkKTMGQSSlDmDQJIyZxBIUuYMAknKnEEgSZkzCCQpcwaBJGXOIJCkzBkEkpQ5g0CSMmcQSFLmDAJJypxBIEmZMwgkKXMGgSRlziCQpMwZBJKUOYNAkjLXJUEQEeMjYm1ENEbErCLzIyJuLcxfERHHF9qHRMTTEdEQEasjYkZX1CNJKl3ZQRARvYG5wARgBHBORIxo020CMKxwmw7cXmjfBvzPlNLngBOBS4osK0nqRl1xRDAGaEwprUspbQUWAVPa9JkC3J1aPA8cEhGDUkobU0ovA6SU3gEagMFdUJMkqURdEQSDgTdbTTfx0f/Md9snIqqB44AXuqAmSVKJuiIIokhb6kyfiDgIWAxcllJ6u+hGIqZHRF1E1DU3N+9xsZKkXXVFEDQBQ1pNVwEbSu0TEX1oCYGFKaV/bW8jKaX5KaXalFLtYYcd1gVlS5Kga4LgJWBYRNRERF9gKrCkTZ8lwPmFq4dOBLaklDZGRAB3AA0ppX/qglokSZ20X7krSClti4hLgceA3sCdKaXVEXFxYf48YCkwEWgE/gpcWFh8LPBtYGVE1BfafphSWlpuXZKk0kRKbU/n7/1qa2tTXV1dpcuQpH1KRCxLKdW2bfebxZKUOYNAkjJnEEhS5gwCScqcQSBJmTMIJClzBoEkZc4gkKTMGQSSlDmDQJIyZxBIUuYMAknKnEEgSZkzCCQpcwaBJGXOIJCkzBkEkpQ5g0CSMmcQSFLmDAJJypxBIEmZMwgkKXMGgSRlziCQpMwZBJKUOYNAkjJnEEhS5gwCScqcQSBJmTMIJClzBoEkZa5LgiAixkfE2ohojIhZReZHRNxamL8iIo4vdVlJUvcqOwgiojcwF5gAjADOiYgRbbpNAIYVbtOB2zuxrCSpG+3XBesYAzSmlNYBRMQiYArwaqs+U4C7U0oJeD4iDomIQUB1Cct2mWseWc2rG97ujlVLUo8YccTBzPmHkV26zq44NTQYeLPVdFOhrZQ+pSwLQERMj4i6iKhrbm4uu2hJUouuOCKIIm2pxD6lLNvSmNJ8YD5AbW1t0T6709UpKkkfB10RBE3AkFbTVcCGEvv0LWFZSVI36opTQy8BwyKiJiL6AlOBJW36LAHOL1w9dCKwJaW0scRlJUndqOwjgpTStoi4FHgM6A3cmVJaHREXF+bPA5YCE4FG4K/AhR0tW25NkqTSRcuFPPuW2traVFdXV+kyJGmfEhHLUkq1bdv9ZrEkZc4gkKTMGQSSlDmDQJIyZxBIUuYMAknKnEEgSZkzCCQpcwaBJGXOIJCkzBkEkpQ5g0CSMmcQSFLmDAJJypxBIEmZMwgkKXMGgSRlziCQpMwZBJKUOYNAkjJnEEhS5gwCScqcQSBJmTMIJClzBoEkZc4gkKTMGQSSlDmDQJIyZxBIUuYMAknKXFlBEBGfiojHI+L1wv0n2+k3PiLWRkRjRMxq1X5jRKyJiBUR8WBEHFJOPZKkziv3iGAW8GRKaRjwZGF6FxHRG5gLTABGAOdExIjC7MeBo1NKxwCvAbPLrEeS1EnlBsEU4K7C47uAbxbpMwZoTCmtSyltBRYVliOl9G8ppW2Ffs8DVWXWI0nqpHKD4PCU0kaAwv2ni/QZDLzZarqp0NbWd4D/U2Y9kqRO2m93HSLiCeDvisy6ssRtRJG21GYbVwLbgIUd1DEdmA7wmc98psRNS5J2Z7dBkFL6envzIuJPETEopbQxIgYBm4p0awKGtJquAja0Wsc0YBJwakop0Y6U0nxgPkBtbW27/SRJnVPuqaElwLTC42nAw0X6vAQMi4iaiOgLTC0sR0SMB64AJqeU/lpmLZKkPVBuENwAjIuI14FxhWki4oiIWApQ+DD4UuAxoAG4P6W0urD8z4EBwOMRUR8R88qsR5LUSbs9NdSRlNJm4NQi7RuAia2mlwJLi/T7r+VsX5JUPr9ZLEmZMwgkKXMGgSRlziCQpMwZBJKUOYNAkjJnEEhS5gwCScqcQSBJmTMIJClzBoEkZc4gkKTMGQSSlDmDQJIyZxBIUuYMAknKnEEgSZkzCCQpcwaBJGXOIJCkzBkEkpQ5g0CSMmcQSFLmDAJJypxBIEmZMwgkKXMGgSRlziCQpMwZBJKUOYNAkjJnEEhS5soKgoj4VEQ8HhGvF+4/2U6/8RGxNiIaI2JWkfkzIyJFxKHl1CNJ6rxyjwhmAU+mlIYBTxamdxERvYG5wARgBHBORIxoNX8IMA74Y5m1SJL2QLlBMAW4q/D4LuCbRfqMARpTSutSSluBRYXldvgZcDmQyqxFkrQHyg2Cw1NKGwEK958u0mcw8Gar6aZCGxExGXgrpbR8dxuKiOkRURcRdc3NzWWWLUnaYb/ddYiIJ4C/KzLryhK3EUXaUkT0L6zjtFJWklKaD8wHqK2t9ehBkrrIboMgpfT19uZFxJ8iYlBKaWNEDAI2FenWBAxpNV0FbACGAjXA8ojY0f5yRIxJKf3fTuyDJKkM5Z4aWgJMKzyeBjxcpM9LwLCIqImIvsBUYElKaWVK6dMppeqUUjUtgXG8ISBJPavcILgBGBcRr9Ny5c8NABFxREQsBUgpbQMuBR4DGoD7U0qry9yuJKmL7PbUUEdSSpuBU4u0bwAmtppeCizdzbqqy6lFkrRn/GaxJGXOIJCkzBkEkpQ5g0CSMmcQSFLmDAJJypxBIEmZMwgkKXMGgSRlziCQpMwZBJKUOYNAkjJnEEhS5gwCScqcQSBJmTMIJClzBoEkZc4gkKTMGQSSlDmDQJIyZxBIUuYMAknKnEEgSZkzCCQpc5FSqnQNnRYRzcC/7+HihwL/0YXldBXr6hzr6hzr6py9tS4or7b/klI6rG3jPhkE5YiIupRSbaXraMu6Ose6Ose6OmdvrQu6pzZPDUlS5gwCScpcjkEwv9IFtMO6Ose6Ose6OmdvrQu6obbsPiOQJO0qxyMCSVIrBoEkZS6rIIiI8RGxNiIaI2JWhWtZHxErI6I+IuoKbZ+KiMcj4vXC/Sd7oI47I2JTRKxq1dZuHRExuzB+ayPiGz1c19UR8VZhzOojYmIF6hoSEU9HRENErI6IGYX2io5ZB3VVdMwiol9EvBgRywt1XVNor/R4tVfX3vAc6x0Rr0TEo4Xp7h+rlFIWN6A38AZwJNAXWA6MqGA964FD27T9BJhVeDwL+MceqOMrwPHAqt3VAYwojNv+QE1hPHv3YF1XAzOL9O3JugYBxxceDwBeK2y/omPWQV0VHTMggIMKj/sALwAn7gXj1V5de8Nz7H8A9wKPFqa7faxyOiIYAzSmlNallLYCi4ApFa6prSnAXYXHdwHf7O4NppR+A/y5xDqmAItSSn9LKf0BaKRlXHuqrvb0ZF0bU0ovFx6/AzQAg6nwmHVQV3t6qq6UUnq3MNmncEtUfrzaq6s9PVJXRFQBfw/8c5ttd+tY5RQEg4E3W0030fELpbsl4N8iYllETC+0HZ5S2ggtL2zg0xWqrb069oYxvDQiVhROHe04RK5IXRFRDRxHy7vJvWbM2tQFFR6zwqmOemAT8HhKaa8Yr3bqgsqO183A5cD2Vm3dPlY5BUEUaavktbNjU0rHAxOASyLiKxWspVSVHsPbgaHAaGAj8NNCe4/XFREHAYuBy1JKb3fUtUhbt9VWpK6Kj1lK6cOU0migChgTEUd30L3SdVVsvCJiErAppbSs1EWKtO1RTTkFQRMwpNV0FbChQrWQUtpQuN8EPEjLId2fImIQQOF+U4XKa6+Oio5hSulPhRfvduAX/OdhcI/WFRF9aPnPdmFK6V8LzRUfs2J17S1jVqjl/wHPAOPZC8arWF0VHq+xwOSIWE/LqetTIuJf6IGxyikIXgKGRURNRPQFpgJLKlFIRBwYEQN2PAZOA1YV6plW6DYNeLgS9XVQxxJgakTsHxE1wDDgxZ4qaseLoeC/0TJmPVpXRARwB9CQUvqnVrMqOmbt1VXpMYuIwyLikMLjA4CvA2uo/HgVrauS45VSmp1SqkopVdPy/9NTKaXz6Imx6o5PvffWGzCRlqsp3gCurGAdR9Lyaf9yYPWOWoCBwJPA64X7T/VALf+blkPgD2h5h/HfO6oDuLIwfmuBCT1c1z3ASmBF4UUwqAJ1nUTL4fcKoL5wm1jpMeugroqOGXAM8Eph+6uA/7W753qF66r4c6ywrZP5z6uGun2s/IkJScpcTqeGJElFGASSlDmDQJIyZxBIUuYMAknKnEEgSZkzCCQpc/8fPP+B7KL60AsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1=torch.linspace(0,400,100)\n",
    "y1=sample_from_constraints_dict(dict_soft_constraints=vae.dict_soft_constraints,\n",
    "                                var_name=\"mask_volume_absolute\", \n",
    "                                var_value=x1, \n",
    "                                verbose=False)\n",
    "plt.plot(x1,y1, label=\"mask_volume_absolute\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are 3 possible simulation types: scratch, resumed, pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation type = scratch\n"
     ]
    }
   ],
   "source": [
    "print(\"simulation type = \"+str(params[\"simulation\"][\"type\"]))\n",
    "\n",
    "    \n",
    "if (params[\"simulation\"][\"type\"] == \"scratch\"):\n",
    "    \n",
    "    epoch_restart = -1\n",
    "    history_dict = {}\n",
    "    min_test_loss = 99999999\n",
    "\n",
    "elif (params[\"simulation\"][\"type\"] == \"resume\"):\n",
    "    \n",
    "    \n",
    "    resumed = file2resumed(path=ckpt_file, device=None)\n",
    "    #resumed = file2resumed(path=ckpt_file, device='cpu')\n",
    "        \n",
    "    load_model_optimizer(resumed=resumed,  \n",
    "                         model=vae,\n",
    "                         optimizer=optimizer,\n",
    "                         overwrite_member_var=True)\n",
    "    \n",
    "    ckp = load_info(resumed=resumed, \n",
    "                    load_epoch=True, \n",
    "                    load_history=True)\n",
    "    \n",
    "    epoch_restart = ckp.epoch\n",
    "    history_dict = ckp.history_dict\n",
    "    min_test_loss = min(history_dict[\"test_loss\"])\n",
    "    \n",
    "    \n",
    "elif (params[\"simulation\"][\"type\"] == \"pretrained\"):\n",
    "    \n",
    "    resumed = file2resumed(path=ckpt_file, device=None)\n",
    "    # resumed = file2resumed(path=ckpt_file, device='cpu')\n",
    "        \n",
    "    load_model_optimizer(resumed=resumed,  \n",
    "                         model=vae,\n",
    "                         optimizer=None,\n",
    "                         overwrite_member_var=False)\n",
    "       \n",
    "    epoch_restart = -1\n",
    "    history_dict = {}\n",
    "    min_test_loss = 99999999\n",
    "    \n",
    "else:\n",
    "    raise Exception(\"simulation type is NOT recognized\")\n",
    "    \n",
    "# instantiate the scheduler if necessary    \n",
    "if params[\"optimizer\"][\"scheduler_is_active\"]:\n",
    "    scheduler = instantiate_scheduler(optimizer=optimizer, dict_params_scheduler=params[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__comment': 'if active=false use ELBO, else use GECO with Log-Likelihood threshold = n_pixels * n_channel * threshold', 'is_active': True, 'factor_balance_range': [0.1, 0.7, 0.8], 'factor_sparsity_range': [-1, 1.0, 100], 'target_fg_fraction': [0.1, 0.25], 'target_mse': [0.75, 1.0], 'bg_std': 0.03, 'fg_std': 0.03}\n"
     ]
    }
   ],
   "source": [
    "vae.geco_dict = params[\"GECO_loss\"]\n",
    "print(vae.geco_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU GB -> 0.127860224\n"
     ]
    }
   ],
   "source": [
    "TEST_FREQUENCY = params[\"simulation\"][\"TEST_FREQUENCY\"]\n",
    "CHECKPOINT_FREQUENCY = params[\"simulation\"][\"CHECKPOINT_FREQUENCY\"]\n",
    "NUM_EPOCHS = params[\"simulation\"][\"MAX_EPOCHS\"]\n",
    "print(\"GPU GB ->\",torch.cuda.memory_allocated()/1E9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =   0 train_loss=42.70375\n",
      "i =   1 train_loss=39.38717\n",
      "Train [epoch    0] loss=41.045, mse=41.148, reg=2.499, kl_tot=2.840, sparsity=9.621, fg_fraction=0.179, geco_sp=1.000, geco_bal=0.700 prob_factor=0.5000\n",
      "i =   0 train_loss=25.95178\n",
      "i =   1 train_loss=27.22672\n",
      "i =   2 train_loss=27.22805\n",
      "i =   3 train_loss=26.37268\n",
      "Test  [epoch    0] loss=26.695, mse=17.591, reg=0.387, kl_tot=42.199, sparsity=1.499, fg_fraction=0.042, geco_sp=1.000, geco_bal=0.702 prob_factor 0.5000\n",
      "saved files -> NEW_ARCHIVE/merfish_res4_aug_27_v2/ckp_0.pkl  NEW_ARCHIVE/merfish_res4_aug_27_v2/history_0.pkl\n",
      "Train [epoch    1] loss=30.611, mse=25.247, reg=1.230, kl_tot=22.742, sparsity=5.236, fg_fraction=0.104, geco_sp=1.000, geco_bal=0.702 prob_factor=0.5000\n",
      "Train [epoch    2] loss=27.095, mse=21.906, reg=1.677, kl_tot=4.853, sparsity=9.065, fg_fraction=0.146, geco_sp=0.999, geco_bal=0.704 prob_factor=0.5000\n",
      "Train [epoch    3] loss=25.929, mse=16.941, reg=0.506, kl_tot=17.171, sparsity=8.581, fg_fraction=0.082, geco_sp=0.998, geco_bal=0.706 prob_factor=0.5000\n",
      "Train [epoch    4] loss=20.410, mse=16.212, reg=0.324, kl_tot=6.972, sparsity=6.689, fg_fraction=0.065, geco_sp=0.997, geco_bal=0.708 prob_factor=0.5000\n",
      "Train [epoch    5] loss=17.534, mse=15.834, reg=0.142, kl_tot=15.234, sparsity=1.786, fg_fraction=0.034, geco_sp=0.995, geco_bal=0.710 prob_factor=0.5000\n",
      "Train [epoch    6] loss=14.644, mse=17.307, reg=0.182, kl_tot=2.026, sparsity=1.632, fg_fraction=0.039, geco_sp=0.994, geco_bal=0.711 prob_factor=0.5000\n",
      "Train [epoch    7] loss=14.671, mse=16.924, reg=0.211, kl_tot=1.593, sparsity=2.018, fg_fraction=0.043, geco_sp=0.992, geco_bal=0.713 prob_factor=0.5000\n",
      "Train [epoch    8] loss=14.708, mse=17.300, reg=0.180, kl_tot=1.963, sparsity=1.676, fg_fraction=0.038, geco_sp=0.990, geco_bal=0.714 prob_factor=0.5000\n",
      "Train [epoch    9] loss=14.326, mse=16.768, reg=0.153, kl_tot=3.062, sparsity=1.357, fg_fraction=0.032, geco_sp=0.988, geco_bal=0.716 prob_factor=0.5000\n",
      "Train [epoch   10] loss=14.301, mse=16.656, reg=0.141, kl_tot=3.454, sparsity=1.288, fg_fraction=0.032, geco_sp=0.985, geco_bal=0.718 prob_factor=0.5000\n",
      "Train [epoch   11] loss=13.654, mse=16.098, reg=0.132, kl_tot=2.472, sparsity=1.307, fg_fraction=0.030, geco_sp=0.983, geco_bal=0.719 prob_factor=0.5000\n",
      "Train [epoch   12] loss=12.956, mse=15.289, reg=0.137, kl_tot=1.743, sparsity=1.373, fg_fraction=0.031, geco_sp=0.981, geco_bal=0.721 prob_factor=0.5000\n",
      "Train [epoch   13] loss=13.785, mse=16.478, reg=0.134, kl_tot=1.479, sparsity=1.400, fg_fraction=0.031, geco_sp=0.979, geco_bal=0.723 prob_factor=0.5000\n",
      "Train [epoch   14] loss=13.595, mse=16.278, reg=0.141, kl_tot=1.453, sparsity=1.335, fg_fraction=0.031, geco_sp=0.976, geco_bal=0.724 prob_factor=0.5000\n",
      "Train [epoch   15] loss=11.826, mse=13.828, reg=0.162, kl_tot=1.578, sparsity=1.272, fg_fraction=0.033, geco_sp=0.974, geco_bal=0.726 prob_factor=0.5000\n",
      "Train [epoch   16] loss=12.946, mse=15.373, reg=0.159, kl_tot=1.542, sparsity=1.262, fg_fraction=0.033, geco_sp=0.972, geco_bal=0.727 prob_factor=0.5000\n",
      "Train [epoch   17] loss=14.232, mse=17.153, reg=0.168, kl_tot=1.283, sparsity=1.294, fg_fraction=0.034, geco_sp=0.970, geco_bal=0.729 prob_factor=0.5000\n",
      "Train [epoch   18] loss=13.657, mse=16.323, reg=0.192, kl_tot=1.092, sparsity=1.337, fg_fraction=0.037, geco_sp=0.967, geco_bal=0.731 prob_factor=0.5000\n",
      "Train [epoch   19] loss=13.174, mse=15.639, reg=0.203, kl_tot=1.086, sparsity=1.328, fg_fraction=0.036, geco_sp=0.965, geco_bal=0.732 prob_factor=0.5000\n",
      "Train [epoch   20] loss=12.581, mse=14.782, reg=0.223, kl_tot=1.163, sparsity=1.305, fg_fraction=0.038, geco_sp=0.963, geco_bal=0.734 prob_factor=0.5000\n",
      "Train [epoch   21] loss=11.518, mse=13.309, reg=0.243, kl_tot=1.061, sparsity=1.318, fg_fraction=0.043, geco_sp=0.961, geco_bal=0.736 prob_factor=0.5000\n",
      "Train [epoch   22] loss=15.162, mse=18.252, reg=0.241, kl_tot=0.995, sparsity=1.320, fg_fraction=0.037, geco_sp=0.959, geco_bal=0.737 prob_factor=0.5000\n",
      "Train [epoch   23] loss=12.859, mse=15.099, reg=0.251, kl_tot=1.049, sparsity=1.299, fg_fraction=0.041, geco_sp=0.957, geco_bal=0.739 prob_factor=0.5000\n",
      "Train [epoch   24] loss=14.309, mse=17.038, reg=0.248, kl_tot=1.023, sparsity=1.299, fg_fraction=0.039, geco_sp=0.954, geco_bal=0.741 prob_factor=0.5000\n",
      "Train [epoch   25] loss=14.481, mse=17.197, reg=0.283, kl_tot=1.058, sparsity=1.293, fg_fraction=0.040, geco_sp=0.952, geco_bal=0.742 prob_factor=0.5000\n",
      "Train [epoch   26] loss=12.284, mse=14.160, reg=0.321, kl_tot=1.128, sparsity=1.284, fg_fraction=0.044, geco_sp=0.950, geco_bal=0.744 prob_factor=0.5000\n",
      "Train [epoch   27] loss=13.513, mse=15.803, reg=0.307, kl_tot=1.082, sparsity=1.288, fg_fraction=0.042, geco_sp=0.948, geco_bal=0.746 prob_factor=0.5000\n",
      "Train [epoch   28] loss=14.087, mse=16.523, reg=0.317, kl_tot=1.123, sparsity=1.283, fg_fraction=0.041, geco_sp=0.946, geco_bal=0.748 prob_factor=0.5000\n",
      "Train [epoch   29] loss=12.228, mse=14.005, reg=0.320, kl_tot=1.166, sparsity=1.273, fg_fraction=0.042, geco_sp=0.944, geco_bal=0.749 prob_factor=0.5000\n",
      "Train [epoch   30] loss=12.274, mse=14.025, reg=0.323, kl_tot=1.217, sparsity=1.268, fg_fraction=0.042, geco_sp=0.942, geco_bal=0.751 prob_factor=0.5000\n",
      "Train [epoch   31] loss=12.984, mse=14.887, reg=0.350, kl_tot=1.302, sparsity=1.270, fg_fraction=0.044, geco_sp=0.940, geco_bal=0.753 prob_factor=0.5000\n",
      "Train [epoch   32] loss=12.272, mse=13.905, reg=0.369, kl_tot=1.235, sparsity=1.279, fg_fraction=0.044, geco_sp=0.938, geco_bal=0.754 prob_factor=0.5000\n",
      "Train [epoch   33] loss=12.609, mse=14.329, reg=0.344, kl_tot=1.433, sparsity=1.246, fg_fraction=0.045, geco_sp=0.936, geco_bal=0.756 prob_factor=0.5000\n",
      "Train [epoch   34] loss=13.104, mse=14.896, reg=0.375, kl_tot=1.348, sparsity=1.291, fg_fraction=0.046, geco_sp=0.934, geco_bal=0.758 prob_factor=0.5000\n",
      "Train [epoch   35] loss=12.300, mse=13.734, reg=0.387, kl_tot=1.636, sparsity=1.269, fg_fraction=0.045, geco_sp=0.932, geco_bal=0.759 prob_factor=0.5000\n",
      "Train [epoch   36] loss=13.996, mse=15.850, reg=0.408, kl_tot=1.679, sparsity=1.314, fg_fraction=0.047, geco_sp=0.930, geco_bal=0.761 prob_factor=0.5000\n",
      "Train [epoch   37] loss=13.032, mse=14.552, reg=0.446, kl_tot=1.567, sparsity=1.314, fg_fraction=0.049, geco_sp=0.928, geco_bal=0.763 prob_factor=0.5000\n",
      "Train [epoch   38] loss=11.795, mse=12.890, reg=0.456, kl_tot=1.456, sparsity=1.349, fg_fraction=0.048, geco_sp=0.927, geco_bal=0.764 prob_factor=0.5000\n",
      "Train [epoch   39] loss=14.168, mse=15.968, reg=0.441, kl_tot=1.500, sparsity=1.349, fg_fraction=0.046, geco_sp=0.925, geco_bal=0.766 prob_factor=0.5000\n",
      "Train [epoch   40] loss=13.128, mse=14.539, reg=0.490, kl_tot=1.368, sparsity=1.379, fg_fraction=0.049, geco_sp=0.923, geco_bal=0.768 prob_factor=0.5000\n",
      "Train [epoch   41] loss=11.484, mse=12.379, reg=0.498, kl_tot=1.227, sparsity=1.405, fg_fraction=0.050, geco_sp=0.921, geco_bal=0.769 prob_factor=0.5000\n",
      "Train [epoch   42] loss=13.162, mse=14.161, reg=0.651, kl_tot=1.533, sparsity=1.513, fg_fraction=0.053, geco_sp=0.919, geco_bal=0.771 prob_factor=0.5000\n",
      "Train [epoch   43] loss=12.154, mse=12.959, reg=0.551, kl_tot=1.715, sparsity=1.444, fg_fraction=0.052, geco_sp=0.917, geco_bal=0.773 prob_factor=0.5000\n",
      "Train [epoch   44] loss=12.696, mse=13.549, reg=0.565, kl_tot=1.621, sparsity=1.531, fg_fraction=0.054, geco_sp=0.915, geco_bal=0.774 prob_factor=0.5000\n",
      "Train [epoch   45] loss=12.067, mse=12.129, reg=0.732, kl_tot=1.629, sparsity=1.886, fg_fraction=0.061, geco_sp=0.914, geco_bal=0.776 prob_factor=0.5000\n",
      "Train [epoch   46] loss=13.239, mse=14.038, reg=0.341, kl_tot=4.081, sparsity=1.262, fg_fraction=0.041, geco_sp=0.912, geco_bal=0.778 prob_factor=0.5000\n",
      "Train [epoch   47] loss=12.602, mse=12.987, reg=0.573, kl_tot=2.808, sparsity=1.556, fg_fraction=0.053, geco_sp=0.910, geco_bal=0.779 prob_factor=0.5000\n",
      "Train [epoch   48] loss=11.385, mse=11.003, reg=0.831, kl_tot=1.784, sparsity=1.932, fg_fraction=0.060, geco_sp=0.908, geco_bal=0.781 prob_factor=0.5000\n",
      "Train [epoch   49] loss=11.867, mse=12.312, reg=0.528, kl_tot=1.814, sparsity=1.575, fg_fraction=0.047, geco_sp=0.907, geco_bal=0.782 prob_factor=0.5000\n",
      "Train [epoch   50] loss=12.292, mse=12.944, reg=0.516, kl_tot=1.808, sparsity=1.493, fg_fraction=0.044, geco_sp=0.905, geco_bal=0.784 prob_factor=0.5000\n",
      "Train [epoch   51] loss=13.208, mse=13.424, reg=0.715, kl_tot=1.814, sparsity=1.897, fg_fraction=0.056, geco_sp=0.903, geco_bal=0.785 prob_factor=0.5000\n",
      "Train [epoch   52] loss=10.925, mse=10.344, reg=0.700, kl_tot=2.330, sparsity=1.928, fg_fraction=0.055, geco_sp=0.901, geco_bal=0.787 prob_factor=0.5000\n",
      "Train [epoch   53] loss=11.222, mse=10.591, reg=0.674, kl_tot=3.649, sparsity=1.743, fg_fraction=0.049, geco_sp=0.900, geco_bal=0.788 prob_factor=0.5000\n",
      "Train [epoch   54] loss=11.494, mse=11.178, reg=0.542, kl_tot=2.612, sparsity=1.879, fg_fraction=0.053, geco_sp=0.898, geco_bal=0.790 prob_factor=0.5000\n",
      "Train [epoch   55] loss=10.410, mse=9.576, reg=0.701, kl_tot=2.074, sparsity=2.057, fg_fraction=0.058, geco_sp=0.896, geco_bal=0.791 prob_factor=0.5000\n",
      "Train [epoch   56] loss=11.180, mse=10.643, reg=0.606, kl_tot=2.711, sparsity=1.902, fg_fraction=0.062, geco_sp=0.895, geco_bal=0.793 prob_factor=0.5000\n",
      "Train [epoch   57] loss=11.202, mse=10.154, reg=0.756, kl_tot=3.608, sparsity=2.009, fg_fraction=0.069, geco_sp=0.893, geco_bal=0.794 prob_factor=0.5000\n",
      "Train [epoch   58] loss=9.636, mse=8.616, reg=0.669, kl_tot=2.794, sparsity=1.883, fg_fraction=0.068, geco_sp=0.891, geco_bal=0.796 prob_factor=0.5000\n",
      "Train [epoch   59] loss=9.086, mse=8.057, reg=0.561, kl_tot=2.715, sparsity=1.874, fg_fraction=0.073, geco_sp=0.890, geco_bal=0.797 prob_factor=0.5000\n",
      "Train [epoch   60] loss=9.281, mse=7.996, reg=0.586, kl_tot=3.003, sparsity=2.055, fg_fraction=0.085, geco_sp=0.888, geco_bal=0.798 prob_factor=0.5000\n",
      "Train [epoch   61] loss=9.022, mse=7.647, reg=0.614, kl_tot=3.108, sparsity=2.024, fg_fraction=0.085, geco_sp=0.887, geco_bal=0.799 prob_factor=0.5000\n",
      "Train [epoch   62] loss=9.378, mse=8.011, reg=0.661, kl_tot=3.321, sparsity=2.004, fg_fraction=0.093, geco_sp=0.886, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   63] loss=8.364, mse=7.007, reg=0.453, kl_tot=3.115, sparsity=2.004, fg_fraction=0.087, geco_sp=0.885, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   64] loss=9.890, mse=8.368, reg=0.839, kl_tot=3.363, sparsity=2.094, fg_fraction=0.099, geco_sp=0.884, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   65] loss=10.005, mse=8.174, reg=1.078, kl_tot=3.369, sparsity=2.184, fg_fraction=0.110, geco_sp=0.883, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   66] loss=8.526, mse=7.185, reg=0.529, kl_tot=2.995, sparsity=1.988, fg_fraction=0.089, geco_sp=0.883, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   67] loss=9.584, mse=7.727, reg=1.103, kl_tot=3.087, sparsity=2.156, fg_fraction=0.110, geco_sp=0.882, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   68] loss=10.157, mse=8.513, reg=0.999, kl_tot=3.236, sparsity=2.156, fg_fraction=0.097, geco_sp=0.882, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   69] loss=9.012, mse=7.190, reg=1.027, kl_tot=3.098, sparsity=2.064, fg_fraction=0.092, geco_sp=0.881, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   70] loss=8.713, mse=7.319, reg=0.610, kl_tot=3.181, sparsity=1.969, fg_fraction=0.083, geco_sp=0.881, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   71] loss=9.448, mse=7.733, reg=0.998, kl_tot=2.739, sparsity=2.176, fg_fraction=0.106, geco_sp=0.880, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   72] loss=8.117, mse=6.562, reg=0.766, kl_tot=2.379, sparsity=2.021, fg_fraction=0.099, geco_sp=0.880, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   73] loss=8.888, mse=7.131, reg=0.905, kl_tot=3.537, sparsity=1.991, fg_fraction=0.092, geco_sp=0.880, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   74] loss=8.975, mse=7.475, reg=0.604, kl_tot=3.584, sparsity=2.043, fg_fraction=0.085, geco_sp=0.879, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   75] loss=9.925, mse=7.901, reg=1.074, kl_tot=2.846, sparsity=2.475, fg_fraction=0.116, geco_sp=0.879, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   76] loss=7.967, mse=6.390, reg=0.714, kl_tot=2.247, sparsity=2.087, fg_fraction=0.099, geco_sp=0.879, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   77] loss=8.304, mse=6.866, reg=0.669, kl_tot=2.616, sparsity=1.996, fg_fraction=0.101, geco_sp=0.878, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   78] loss=8.939, mse=7.377, reg=0.550, kl_tot=3.467, sparsity=2.169, fg_fraction=0.085, geco_sp=0.878, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   79] loss=9.093, mse=6.670, reg=0.948, kl_tot=3.425, sparsity=2.637, fg_fraction=0.111, geco_sp=0.878, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   80] loss=8.976, mse=7.173, reg=0.880, kl_tot=2.738, sparsity=2.264, fg_fraction=0.116, geco_sp=0.877, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   81] loss=7.957, mse=6.715, reg=0.569, kl_tot=2.437, sparsity=1.872, fg_fraction=0.100, geco_sp=0.877, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   82] loss=7.999, mse=6.705, reg=0.453, kl_tot=2.769, sparsity=1.959, fg_fraction=0.095, geco_sp=0.877, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   83] loss=8.075, mse=6.357, reg=0.529, kl_tot=2.969, sparsity=2.250, fg_fraction=0.095, geco_sp=0.877, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   84] loss=8.700, mse=6.710, reg=0.738, kl_tot=3.138, sparsity=2.412, fg_fraction=0.110, geco_sp=0.877, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   85] loss=8.966, mse=7.124, reg=0.953, kl_tot=2.974, sparsity=2.179, fg_fraction=0.116, geco_sp=0.876, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   86] loss=7.771, mse=6.352, reg=0.585, kl_tot=2.736, sparsity=1.911, fg_fraction=0.104, geco_sp=0.876, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   87] loss=9.501, mse=8.085, reg=0.704, kl_tot=3.437, sparsity=2.034, fg_fraction=0.104, geco_sp=0.876, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   88] loss=8.094, mse=6.349, reg=0.548, kl_tot=2.956, sparsity=2.266, fg_fraction=0.103, geco_sp=0.876, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   89] loss=8.565, mse=6.660, reg=0.800, kl_tot=2.660, sparsity=2.358, fg_fraction=0.109, geco_sp=0.876, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   90] loss=7.834, mse=6.166, reg=0.630, kl_tot=2.713, sparsity=2.118, fg_fraction=0.110, geco_sp=0.876, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   91] loss=7.446, mse=5.956, reg=0.453, kl_tot=2.930, sparsity=1.978, fg_fraction=0.104, geco_sp=0.876, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   92] loss=8.072, mse=6.449, reg=0.507, kl_tot=3.409, sparsity=2.085, fg_fraction=0.116, geco_sp=0.876, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   93] loss=7.511, mse=5.923, reg=0.461, kl_tot=2.876, sparsity=2.088, fg_fraction=0.104, geco_sp=0.875, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   94] loss=8.260, mse=6.506, reg=0.662, kl_tot=2.817, sparsity=2.241, fg_fraction=0.117, geco_sp=0.875, geco_bal=0.800 prob_factor=0.5000\n",
      "Train [epoch   95] loss=7.801, mse=6.267, reg=0.455, kl_tot=2.822, sparsity=2.124, fg_fraction=0.109, geco_sp=0.875, geco_bal=0.800 prob_factor=0.5000\n"
     ]
    }
   ],
   "source": [
    "#epoch_restart = 322\n",
    "#epoch_restart = 11518\n",
    "#epoch_restart = 783\n",
    "for delta_epoch in range(1,NUM_EPOCHS+1):\n",
    "    epoch = delta_epoch+epoch_restart    \n",
    "    \n",
    "    vae.prob_corr_factor=linear_interpolation(epoch, \n",
    "                                              values=params[\"shortcut_prob_corr_factor\"][\"values\"],\n",
    "                                              times=params[\"shortcut_prob_corr_factor\"][\"times\"])\n",
    "        \n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "    #with torch.autograd.set_detect_anomaly(False):\n",
    "        with torch.enable_grad():\n",
    "            vae.train()\n",
    "            train_metrics = process_one_epoch(model=vae, \n",
    "                                              dataloader=train_loader, \n",
    "                                              optimizer=optimizer, \n",
    "                                              verbose=(epoch==0), \n",
    "                                              weight_clipper=None)\n",
    "        with torch.no_grad():        \n",
    "            s = pretty_print_metrics(epoch, train_metrics, is_train=True)\n",
    "            print(s,\"prob_factor=%.4f\" %(vae.prob_corr_factor))\n",
    "            \n",
    "            history_dict = append_dict_to_dict(source=train_metrics, \n",
    "                                               target=history_dict,\n",
    "                                               prefix_exclude=\"wrong_examples\",\n",
    "                                               prefix_to_add=\"train_\")\n",
    "        \n",
    "    if params[\"optimizer\"][\"scheduler_is_active\"]:\n",
    "        scheduler.step()\n",
    "    \n",
    "    if(epoch % TEST_FREQUENCY == 0):\n",
    "        with torch.no_grad():\n",
    "            vae.eval()\n",
    "            test_metrics = process_one_epoch(model=vae, \n",
    "                                             dataloader=test_loader, \n",
    "                                             optimizer=optimizer, \n",
    "                                             verbose=(epoch==0), \n",
    "                                             weight_clipper=None)\n",
    "        \n",
    "            s = pretty_print_metrics(epoch, test_metrics, is_train=False)\n",
    "            print(s,\"prob_factor %.4f\" %(vae.prob_corr_factor))\n",
    "        \n",
    "            history_dict = append_dict_to_dict(source=train_metrics, \n",
    "                                               target=history_dict,\n",
    "                                               prefix_exclude=\"wrong_examples\",\n",
    "                                               prefix_to_add=\"test_\")\n",
    "        \n",
    "            test_loss = test_metrics[\"loss\"]\n",
    "            min_test_loss = min(min_test_loss, test_loss)\n",
    "            \n",
    "            if((test_loss == min_test_loss) or ((epoch % CHECKPOINT_FREQUENCY) == 0)): \n",
    "                checkpoint_file = os.path.join(dir_output, \"ckp_\"+str(epoch)+\".pkl\")\n",
    "                history_file = os.path.join(dir_output, \"history_\"+str(epoch)+\".pkl\")\n",
    "            \n",
    "                save_everything(model=vae, \n",
    "                                optimizer=optimizer, \n",
    "                                history_dict=history_dict, \n",
    "                                epoch=epoch, \n",
    "                                hyperparams_dict=params, \n",
    "                                path=checkpoint_file)\n",
    "            \n",
    "                save_dict_as_json(history_dict, path=history_file)\n",
    "                print(\"saved files -> \"+checkpoint_file+\"  \"+history_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I HAVE A MEMORY LEAK...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_metrics\n",
    "# history_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test generator after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_in_tmp, labels, index = train_loader.load(batch_size=3)\n",
    "if torch.cuda.is_available():\n",
    "    imgs_in_tmp = imgs_in_tmp.cuda()\n",
    "\n",
    "auch = vae.generate(imgs_in=imgs_in_tmp[:1], draw_boxes=True)\n",
    "\n",
    "pmap_gen = show_batch(auch.inference.p_map[:8], title=\"generated p_map\")\n",
    "imgs_gen = show_batch(auch.imgs[:8], title=\"generated imgs\")\n",
    "display(pmap_gen, imgs_gen)\n",
    "\n",
    "big_mask = auch.inference.big_mask[:,0]\n",
    "big_img = auch.inference.big_img[:,0]\n",
    "tmp = torch.cat((big_mask.expand(-1,big_img.shape[-3],-1,-1), big_img),dim=0)\n",
    "show_batch(tmp, n_col=tmp.shape[0]//5, title=\"masks and imgs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check segmentation WITHOUT tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_in,labels,index = test_loader.load(batch_size=8)\n",
    "if torch.cuda.is_available():\n",
    "    imgs_in = imgs_in.cuda()\n",
    "\n",
    "segmentation = vae.segment(imgs_in)\n",
    "print(segmentation.integer_mask.shape, segmentation.similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(segmentation.raw_image[:4], figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid(segmentation.integer_mask[:4,0].cpu(), figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid(segmentation.fg_prob[:4,0].cpu(), figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check segmentation WITH tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.cum_roi_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae.eval()\n",
    "\n",
    "img_to_segment = train_loader.img[0,:,1000:1300,2100:2400]\n",
    "roi_mask = train_loader.roi_mask[0,:,1000:1300,2100:2400]\n",
    "\n",
    "#img_to_segment = train_loader.img[0]\n",
    "#roi_mask = train_loader.roi_mask[0]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12,12))\n",
    "axs[0].imshow(img_to_segment.permute(1,2,0).squeeze(-1).cpu())\n",
    "axs[1].imshow(roi_mask.permute(1,2,0).squeeze(-1).cpu())\n",
    "axs[0].set_title(\"img to segment\")\n",
    "axs[1].set_title(\"roi mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_to_segment = train_loader.img[0] \n",
    "#img_to_segment = train_loader.img[0,:,1000:1100,1000:1100]\n",
    "#img_to_segment = img_to_segment[:,60:,:40]\n",
    "#del tiling\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU GB ->\",torch.cuda.memory_allocated()/1E9)\n",
    "\n",
    "tiling = vae.segment_with_tiling(single_img=img_to_segment,\n",
    "                                 roi_mask=roi_mask,\n",
    "                                 crop_size=None,\n",
    "                                 stride=(40,40),\n",
    "                                 n_objects_max_per_patch=None,\n",
    "                                 prob_corr_factor=None,\n",
    "                                 overlap_threshold=None,\n",
    "                                 radius_nn=10,\n",
    "                                 batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tiling.similarity._fields)\n",
    "print(tiling.similarity.sparse_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tiling._fields)\n",
    "for fld in tiling._fields:\n",
    "    if isinstance(getattr(tiling, fld),torch.Tensor):\n",
    "        print(fld, getattr(tiling, fld).shape)\n",
    "    else:\n",
    "        print(fld,type(getattr(tiling, fld)))\n",
    "        \n",
    "for fld in tiling.similarity._fields:\n",
    "    if isinstance(getattr(tiling.similarity, fld),torch.Tensor):\n",
    "        print(fld, getattr(tiling.similarity, fld).shape)\n",
    "    else:\n",
    "        print(fld,type(getattr(tiling.similarity, fld)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = tiling.similarity.sparse_matrix.to_dense()\n",
    "# plt.matshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiling_file = \"/Users/ldalessi/DAPI_unsupervised/spacetx-research/TILING_AUG/NEW_tiling_Aug5_radius5.pt\" \n",
    "save_obj(tiling, tiling_file)\n",
    "print(tiling_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiling_original = load_obj(tiling_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(ncols=4, figsize=(24,24))\n",
    "axes[0].imshow(skimage.color.label2rgb(tiling.integer_mask[0,0].cpu().numpy(),\n",
    "                                       numpy.zeros_like(tiling.integer_mask[0,0].cpu().numpy()),\n",
    "                                       alpha=1.0,\n",
    "                                       bg_label=0))\n",
    "axes[1].imshow(skimage.color.label2rgb(tiling.integer_mask[0,0].cpu().numpy(),\n",
    "                                         tiling.raw_image[0,0].cpu().numpy(),\n",
    "                                         alpha=0.25,\n",
    "                                         bg_label=0))\n",
    "axes[2].imshow(tiling.fg_prob[0,0].cpu().numpy(), cmap='gray')\n",
    "axes[3].imshow(tiling.raw_image[0].cpu().permute(1,2,0).squeeze(-1).numpy(), cmap='gray')\n",
    "\n",
    "axes[0].set_title(\"sample integer mask\")\n",
    "axes[1].set_title(\"sample integer mask\")\n",
    "axes[2].set_title(\"fg prob\")\n",
    "axes[3].set_title(\"raw image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seg_mask = vae.segment_with_tiling(train_loader.x[...,2000:2400,2000:2400], \n",
    "#                                   crop_w=80, crop_h=80, \n",
    "#                                   stride_w=60, stride_h=60, n_objects_max_per_patch=10)\n",
    "\n",
    "imgs_in,labels,index = test_loader.load(batch_size=8)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    imgs_in = imgs_in.cuda()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    segmentation = vae.segment(imgs_in)\n",
    "    print(\"segmentation._fields ->\", segmentation._fields)\n",
    "\n",
    "    vae.eval()\n",
    "    output_test = vae.forward(imgs_in,\n",
    "                              draw_image=True,\n",
    "                              draw_bg=True,\n",
    "                              draw_boxes=False,\n",
    "                              verbose=False)\n",
    "    print(\"output_test._fields ->\", output_test._fields)\n",
    "\n",
    "    unet_output = vae.inference_and_generator.unet(imgs_in, verbose=False)\n",
    "    print(\"unet_output._fields ->\", unet_output._fields)\n",
    "\n",
    "    print(imgs_in.shape, segmentation.integer_mask.shape, output_test.imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imgs_in[0,0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen=0\n",
    "ncol = 4\n",
    "ch = unet_output.features.shape[-3]\n",
    "nrows = ch // ncol\n",
    "print(nrows)\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=4, figsize=(24,24))\n",
    "for n in range(ch):\n",
    "    j = n % ncol\n",
    "    i = n // ncol\n",
    "    axs[i,j].imshow((unet_output.features[chosen,n]-imgs_in[0,0]).cpu(), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(unet_output.features[0,4].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(unet_output.features[chosen].unsqueeze(1), n_col=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(unet_output.features[chosen].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_output._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test.inference._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test.inference.prob[...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = show_batch(imgs_in, normalize_range=None)\n",
    "b = show_batch(output_test.imgs, normalize_range=None)\n",
    "display(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen=6\n",
    "figure, axes = plt.subplots(ncols=3, figsize=(24, 24))\n",
    "axes[0].imshow(imgs_in[chosen].cpu().permute(1,2,0).squeeze(-1))  # squeeze only works if singleton dimension\n",
    "axes[1].imshow(skimage.color.label2rgb(skimage.img_as_ubyte(segmentation.integer_mask[chosen,0].cpu()), bg_label=0))\n",
    "axes[2].imshow(output_test.imgs[chosen].cpu().permute(1,2,0).squeeze(-1)) # squeeze only works if singleton dimension\n",
    "axes[0].set_title(\"input image\")\n",
    "axes[1].set_title(\"segmentation mask\")\n",
    "axes[2].set_title(\"reconstructed image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in history_dict.items():\n",
    "    if k.startswith(\"train\"):\n",
    "        print(k,\" -->\", history_dict[k][-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.yscale('log')\n",
    "y_shift=0\n",
    "x_shift=0\n",
    "sign=1\n",
    "plt.plot(np.arange(x_shift, x_shift+len(history_dict[\"train_loss\"])), \n",
    "         sign*np.array(history_dict[\"train_loss\"])+y_shift,'-')\n",
    "plt.plot(np.arange(x_shift, x_shift+len(history_dict[\"test_loss\"])*TEST_FREQUENCY,TEST_FREQUENCY), \n",
    "         sign*np.array(history_dict[\"test_loss\"])+y_shift, '.--')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('LOSS = - ELBO')\n",
    "plt.title('Training procedure')\n",
    "#plt.ylim(ymax=4, ymin=0)\n",
    "plt.grid(True)\n",
    "plt.legend(['train', 'test'])\n",
    "#plt.show()\n",
    "\n",
    "fig_file = os.path.join(dir_output, \"train.png\")\n",
    "plt.savefig(fig_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,len(history_dict[\"train_length_GP\"])), history_dict[\"train_length_GP\"], '-', label=\"train\")\n",
    "plt.plot(np.arange(0,len(history_dict[\"test_length_GP\"])*TEST_FREQUENCY,TEST_FREQUENCY), history_dict[\"test_length_GP\"], 'x', label=\"test\")\n",
    "plt.title('LENGTH GP')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('lenght_GP')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "fig_file = os.path.join(dir_output, \"lenght_GP.png\")\n",
    "plt.savefig(fig_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('fg fraction av', color=color)\n",
    "ax1.plot(np.arange(0, len(history_dict[\"train_fg_fraction\"])),\n",
    "         history_dict[\"train_fg_fraction\"], 'o', color=color, label=\"train\")\n",
    "ax1.plot(np.arange(0, len(history_dict[\"test_fg_fraction\"])*TEST_FREQUENCY, TEST_FREQUENCY),\n",
    "         history_dict[\"test_fg_fraction\"], 'x-', color=color, label=\"test\")\n",
    "\n",
    "ymin=min(params[\"GECO_loss\"][\"target_fg_fraction\"])\n",
    "ymax=max(params[\"GECO_loss\"][\"target_fg_fraction\"])\n",
    "ax1.plot(ymin*np.ones(len(history_dict[\"train_fg_fraction\"])), '-', color='black', label=\"y_min\")\n",
    "ax1.plot(ymax*np.ones(len(history_dict[\"train_fg_fraction\"])), '-', color='black', label=\"y_max\")\n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.grid()\n",
    "#ax1.set_ylim([1000,1870])\n",
    "plt.legend()\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(np.arange(0, len(history_dict[\"train_accuracy\"])),\n",
    "         history_dict[\"train_accuracy\"],'x', color=color)\n",
    "ax2.plot(np.arange(0, len(history_dict[\"test_accuracy\"])*TEST_FREQUENCY, TEST_FREQUENCY),\n",
    "         history_dict[\"test_accuracy\"],'-', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.grid()\n",
    "#ax2.set_ylim([0.97,1.0])\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "fig_file = os.path.join(dir_output, \"accuracy.png\")\n",
    "plt.savefig(fig_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(20,20))\n",
    "ax = f.add_subplot(321)\n",
    "ax2 = f.add_subplot(322)\n",
    "ax3 = f.add_subplot(323)\n",
    "ax4 = f.add_subplot(324)\n",
    "ax5 = f.add_subplot(325)\n",
    "ax6 = f.add_subplot(326)\n",
    "epoch_min, epoch_max = 600, None\n",
    "\n",
    "\n",
    "loss = np.array(history_dict[\"train_loss\"])\n",
    "kl_instance = np.array(history_dict[\"train_kl_instance\"])\n",
    "kl_where = np.array(history_dict[\"train_kl_where\"])\n",
    "kl_logit = np.array(history_dict[\"train_kl_logit\"])\n",
    "kl_raw = np.array(history_dict[\"train_kl_tot\"])\n",
    "mse_raw = np.array(history_dict[\"train_mse\"])\n",
    "reg_raw = np.array(history_dict[\"train_reg\"])\n",
    "sparsity_raw = np.array(history_dict[\"train_sparsity\"])\n",
    "overlap_raw = np.array(history_dict[\"train_cost_overlap\"])\n",
    "f_geco_sparsity = np.array(history_dict[\"train_geco_sparsity\"])\n",
    "f_geco_balance = np.array(history_dict[\"train_geco_balance\"])\n",
    "\n",
    "\n",
    "ax.plot(sparsity_raw,'-',label='sparsity_raw')\n",
    "ax.plot(overlap_raw,'-',label='cost overlap raw')\n",
    "ax.set_xlim([epoch_min, epoch_max])\n",
    "ax.set_ylim([None, 1.01*max(max(sparsity_raw[epoch_min:epoch_max]), max(overlap_raw[epoch_min:epoch_max]))])\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "ax2.plot(kl_instance,'-',label='kl instance raw')\n",
    "ax2.plot(kl_where,'-',label='kl zwhere raw')\n",
    "ax2.set_xlim([epoch_min, epoch_max])\n",
    "ax2.set_ylim([0, 1.01*max(max(kl_instance[epoch_min:epoch_max]),max(kl_where[epoch_min:epoch_max]))])\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "ax3.plot(kl_logit,'-',label='kl logit raw')\n",
    "#ax3.set_ylim([0,1.1])\n",
    "ax3.set_xlim([epoch_min, epoch_max])\n",
    "ax3.set_ylim([None, 1.01*max(kl_logit[epoch_min:epoch_max])])\n",
    "ax3.grid()\n",
    "ax3.legend()\n",
    "\n",
    "\n",
    "ax4.plot(mse_raw ,'x-',label='mse raw')\n",
    "ax4.set_xlim([epoch_min, epoch_max])\n",
    "ax4.set_ylim([None, 1.01*max(mse_raw[epoch_min:epoch_max])])\n",
    "ax4.grid()\n",
    "ax4.legend()\n",
    "\n",
    "\n",
    "ax5.plot(f_geco_balance ,'x-',label='geco_balance')\n",
    "ax5.set_xlim([epoch_min, epoch_max])\n",
    "ax5.set_ylim([None, 1.01*max(f_geco_balance[epoch_min:epoch_max])])\n",
    "ax5.grid()\n",
    "ax5.legend()\n",
    "ax5_v2 = ax5.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:green'\n",
    "ax5_v2.plot(f_geco_sparsity ,'x-',label='geco_sparsity', color=color)\n",
    "ax5_v2.set_ylim([None, 1.01*max(f_geco_sparsity[epoch_min:epoch_max])])\n",
    "ax5_v2.grid()\n",
    "ax5_v2.legend()\n",
    "\n",
    "\n",
    "ax6.plot(loss,'-',label='loss')\n",
    "ax6.plot(f_geco_sparsity * sparsity_raw,'x-',label='scaled_sparsity')\n",
    "ax6.plot(f_geco_balance * reg_raw,'x-',label='scaled_reg')\n",
    "ax6.plot(f_geco_balance * mse_raw,'x-',label='scaled_mse')\n",
    "ax6.plot((1-f_geco_balance) * kl_raw,'x-',label='scaled_kl')\n",
    "ax6.set_ylim([0, 1.01*max(loss[epoch_min:epoch_max])])\n",
    "ax6.set_xlim([epoch_min, epoch_max])\n",
    "ax6.grid()\n",
    "ax6.legend()\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "fig_file = os.path.join(dir_output, \"metrics.png\")\n",
    "plt.savefig(fig_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"GECO_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=20\n",
    "labelsize=20\n",
    "f = plt.figure(figsize=(20,20))\n",
    "ax1 = f.add_subplot(311)\n",
    "ax2 = f.add_subplot(312)\n",
    "ax3 = f.add_subplot(313)\n",
    "epoch_min, epoch_max = 0, None\n",
    "\n",
    "\n",
    "#-----------------------------------\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('epochs', fontsize=fontsize)\n",
    "ax1.set_ylabel('fg_fraction', fontsize=fontsize, color=color)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "ax1.plot(history_dict[\"train_fg_fraction\"], '.--', color=color, label=\"n_object\")\n",
    "ax1.set_xlim([epoch_min, epoch_max])\n",
    "ymin=min(params[\"GECO_loss\"]['target_fg_fraction'])\n",
    "ymax=max(params[\"GECO_loss\"]['target_fg_fraction'])\n",
    "ax1.plot(ymin*np.ones(len(history_dict[\"train_fg_fraction\"])), '-', color='black', label=\"y_min\")\n",
    "ax1.plot(ymax*np.ones(len(history_dict[\"train_fg_fraction\"])), '-', color='black', label=\"y_max\")\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.grid()\n",
    "\n",
    "ax1b = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:green'\n",
    "ax1b.set_xlabel('epochs', fontsize=fontsize)\n",
    "ax1b.set_ylabel('geco_sparsity', color=color, fontsize=fontsize)\n",
    "ax1b.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "plt.plot(history_dict[\"train_geco_sparsity\"],'-',label=\"geco_sparsity\",color=color)\n",
    "ax1b.tick_params(axis='y', labelcolor=color)\n",
    "ax1b.grid()\n",
    "\n",
    "##------------------------------------\n",
    "\n",
    "color = 'tab:red'\n",
    "ax2.set_xlabel('epochs', fontsize=fontsize)\n",
    "ax2.set_ylabel('mse av', fontsize=fontsize, color=color)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "ax2.plot(history_dict[\"train_mse\"], '.--', color=color, label=\"mse av\")\n",
    "ax2.set_xlim([epoch_min, epoch_max])\n",
    "\n",
    "ymin=min(params[\"GECO_loss\"][\"target_mse\"])\n",
    "ymax=max(params[\"GECO_loss\"][\"target_mse\"])\n",
    "ax2.plot(ymin*np.ones(len(history_dict[\"train_mse\"])), '-', color='black', label=\"y_min\")\n",
    "ax2.plot(ymax*np.ones(len(history_dict[\"train_mse\"])), '-', color='black', label=\"y_max\")\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2.grid()\n",
    "\n",
    "ax2b = ax2.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:green'\n",
    "ax2b.set_xlabel('epochs', fontsize=fontsize)\n",
    "ax2b.set_ylabel('geco_balance', fontsize=fontsize, color=color)\n",
    "plt.plot(history_dict[\"train_geco_balance\"],'-',label=\"geco_balance\",color=color)\n",
    "ax2b.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "ax2b.tick_params(axis='y', labelcolor=color)\n",
    "ax2b.grid()\n",
    "\n",
    "##------------------------------------\n",
    "\n",
    "color = 'tab:red'\n",
    "ax3.set_xlabel('epochs', fontsize=fontsize)\n",
    "ax3.set_ylabel('delta_1', fontsize=fontsize, color=color)\n",
    "ax3.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "ax3.plot(history_dict[\"train_delta_1\"], '.--', color=color, label=\"delta_1\")\n",
    "ax3.set_xlim([epoch_min, epoch_max])\n",
    "\n",
    "\n",
    "ax3b = ax3.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:green'\n",
    "ax3b.set_xlabel('epochs', fontsize=fontsize)\n",
    "ax3b.set_ylabel('delta_2', fontsize=fontsize, color=color)\n",
    "plt.plot(history_dict[\"train_delta_2\"],'-',label=\"delta_2\",color=color)\n",
    "ax3b.tick_params(axis='y', labelcolor=color)\n",
    "ax3b.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "ax3b.grid()\n",
    "\n",
    "#-----------------------------------\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "fig_file = os.path.join(dir_output, \"geco.png\")\n",
    "plt.savefig(fig_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of KL vs evidence\n",
    "fontsize=20\n",
    "labelsize=20\n",
    "\n",
    "epoch_min, epoch_max, step = 100, None, 10\n",
    "scale= 1\n",
    "N = len(history_dict[\"train_mse\"][epoch_min:epoch_max:step])\n",
    "colors = np.arange(0.0,N,1.0)/N\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "ax1 = f.add_subplot(221)\n",
    "ax2 = f.add_subplot(222)\n",
    "ax3 = f.add_subplot(223)\n",
    "ax4 = f.add_subplot(224, projection='3d')\n",
    "\n",
    "ax1.set_xlabel('MSE',fontsize=fontsize)\n",
    "ax1.set_ylabel('KL',fontsize=fontsize)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "ax1.scatter(history_dict[\"train_mse\"][epoch_min:epoch_max:step], history_dict[\"train_kl_tot\"][epoch_min:epoch_max:step],c=colors)\n",
    "ax1.plot(history_dict[\"train_mse\"][epoch_min:epoch_max:step], history_dict[\"train_kl_tot\"][epoch_min:epoch_max:step], '--')\n",
    "ax1.grid()\n",
    "#ax1.set_xlim(xmax=2.5)\n",
    "\n",
    "ax2.set_xlabel('SPARSITY',fontsize=fontsize)\n",
    "ax2.set_ylabel('MSE',fontsize=fontsize)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "ax2.scatter(history_dict[\"train_sparsity\"][epoch_min:epoch_max:step], history_dict[\"train_mse\"][epoch_min:epoch_max:step], c=colors)\n",
    "ax2.plot(history_dict[\"train_sparsity\"][epoch_min:epoch_max:step], history_dict[\"train_mse\"][epoch_min:epoch_max:step], '--')\n",
    "ax2.grid()\n",
    "#ax2.set_xlim(xmax=2.5)\n",
    "\n",
    "ax3.set_xlabel('SPARSITY',fontsize=fontsize)\n",
    "ax3.set_ylabel('KL',fontsize=fontsize)\n",
    "ax3.tick_params(axis='both', which='major', labelsize=labelsize)\n",
    "ax3.scatter(history_dict[\"train_sparsity\"][epoch_min:epoch_max:step], history_dict[\"train_kl_tot\"][epoch_min:epoch_max:step], c=colors)\n",
    "ax3.plot(history_dict[\"train_sparsity\"][epoch_min:epoch_max:step], history_dict[\"train_kl_tot\"][epoch_min:epoch_max:step], '--')\n",
    "ax3.grid()\n",
    "#ax3.set_xlim(xmax=2.5)\n",
    "\n",
    "\n",
    "ax4.scatter(history_dict[\"train_kl_tot\"][epoch_min:epoch_max:step],\n",
    "         history_dict[\"train_sparsity\"][epoch_min:epoch_max:step],\n",
    "         history_dict[\"train_mse\"][epoch_min:epoch_max:step], c=colors )\n",
    "\n",
    "ax4.plot(history_dict[\"train_kl_tot\"][epoch_min:epoch_max:step],\n",
    "         history_dict[\"train_sparsity\"][epoch_min:epoch_max:step],\n",
    "         history_dict[\"train_mse\"][epoch_min:epoch_max:step], '--', label='training')\n",
    "ax4.set_xlabel('kl_tot', fontsize=fontsize)\n",
    "ax4.set_ylabel('sparsity', fontsize=fontsize)\n",
    "ax4.set_zlabel('mse', fontsize=fontsize)\n",
    "ax4.legend(prop={'size':fontsize})\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "fig_file = os.path.join(dir_output, \"mse_vs_kll_vs_sparsity.png\")\n",
    "plt.savefig(fig_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run one epoch in eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch=100\n",
    "#load_model_optimizer(path=os.path.join(dir_output, \"ckp_\"+str(epoch)+\".pkl\"), model=vae)\n",
    "\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    test_metrics = process_one_epoch(model=vae, \n",
    "                                     dataloader=test_loader)\n",
    "    print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_img_pkl = os.path.join(dir_output, \"reference.pkl\")\n",
    "tmp_list = [0, 1, 2,3,4,5,6,7,8,9]\n",
    "#tmp_list = [255, 148, 291, 310, 2,3,4,5,6,7,8,9,10]\n",
    "#tmp_list = [425, 411, 61, 194, 91, 384, 339, 54, 336]\n",
    "\n",
    "reference_imgs, labels, index =test_loader.load(index=torch.tensor(tmp_list[:9]))\n",
    "if torch.cuda.is_available():\n",
    "    reference_imgs = reference_imgs.cuda()\n",
    "    \n",
    "save_obj(reference_imgs, ref_img_pkl)\n",
    "\n",
    "reference_imgs = load_obj(ref_img_pkl)\n",
    "b = show_batch(reference_imgs[:],n_col=3,n_padding=4,title=\"REFERENCE\")\n",
    "\n",
    "ref_img_png = os.path.join(dir_output, \"reference.png\")\n",
    "b.savefig(ref_img_png)\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.geco_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen=0\n",
    "with torch.no_grad():\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"--- eval mode ---\")\n",
    "    vae.eval()\n",
    "    output_test = vae.forward(reference_imgs[:],\n",
    "                              draw_image=True,\n",
    "                              draw_boxes=True,\n",
    "                              verbose=True)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"--- train mode ---\")\n",
    "    vae.train()\n",
    "    output_train = vae.forward(reference_imgs[:],\n",
    "                               draw_image=True,\n",
    "                               draw_boxes=True,\n",
    "                               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap_train = show_batch(output_train.inference.p_map, n_col=3,n_padding=4,title=\"Train Prob MAP\")\n",
    "pmap_test = show_batch(output_test.inference.p_map, n_col=3,n_padding=4,title=\"Test Prob MAP\")\n",
    "\n",
    "counts_train = torch.sum(output_train.inference.prob>0.5,dim=0).view(-1).cpu().numpy().tolist()\n",
    "rec_train = show_batch(output_train.imgs[:],n_col=3,n_padding=4,title=\"# rec train \"+str(counts_train))\n",
    "\n",
    "counts_test = torch.sum(output_test.inference.prob>0.5,dim=0).view(-1).cpu().numpy().tolist()\n",
    "rec_test = show_batch(output_test.imgs[:],n_col=3,n_padding=4,title=\"# rec test \"+str(counts_test))\n",
    "\n",
    "background = show_batch(output_train.inference.big_bg,n_col=3,n_padding=4,title=\"BACKGROUND\")\n",
    "reference = show_batch(reference_imgs[:],n_col=3,n_padding=4,title=\"REFERENCE\")\n",
    "\n",
    "background.savefig(os.path.join(dir_output, \"background.png\"))\n",
    "reference.savefig(os.path.join(dir_output, \"reference.png\"))\n",
    "rec_test.savefig(os.path.join(dir_output, \"rec_test.png\"))\n",
    "rec_train.savefig(os.path.join(dir_output, \"rec_train.png\"))\n",
    "pmap_test.savefig(os.path.join(dir_output, \"pmap_test.png\"))\n",
    "pmap_train.savefig(os.path.join(dir_output, \"pmap_train.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(background, reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_train.inference.p_map.sum(dim=(-1,-2,-3)).cpu())\n",
    "print(output_test.inference.p_map.sum(dim=(-1,-2,-3)).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rec_train,reference)\n",
    "display(rec_test,reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pmap_train,reference)\n",
    "display(pmap_test,reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output_train.inference.p_map[chosen,0].cpu().numpy())\n",
    "_ = plt.colorbar()\n",
    "print(torch.topk(output_train.inference.p_map[chosen,0].view(-1), k=10, largest=True, sorted=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output_test.inference.p_map[chosen,0].cpu().numpy())\n",
    "_ = plt.colorbar()\n",
    "print(torch.topk(output_test.inference.p_map[chosen,0].view(-1), k=10, largest=True, sorted=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(output_train.inference.p_map[0,0].view(-1).cpu().numpy(), density=True, bins=50, label=\"pmap_train\")\n",
    "_ = plt.hist(output_test.inference.p_map[0,0].view(-1).cpu().numpy(), density=True, bins=50, label=\"pmap_test\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(dir_output, \"hist_pmap.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize one chosen image in details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output_train\n",
    "how_many_to_show=20\n",
    "counts = torch.sum(output.inference.prob>0.5,dim=0).view(-1).cpu().numpy().tolist()\n",
    "prob_tmp = np.round(output.inference.prob[:how_many_to_show,chosen].view(-1).cpu().numpy(),decimals=4)*10000\n",
    "prob_title = (prob_tmp.astype(int)/10000).tolist()\n",
    "print(\"counts ->\",counts[chosen],\" prob ->\",prob_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = reference_imgs[chosen]\n",
    "ch_out = tmp1.shape[-3]\n",
    "tmp2 = torch.sum(output.inference.big_img[:how_many_to_show,chosen],dim=0).expand(ch_out,-1,-1)\n",
    "tmp3 = torch.sum(output.inference.big_mask[:how_many_to_show,chosen],dim=0).expand(ch_out,-1,-1)\n",
    "mask_times_imgs = output.inference.big_mask * output.inference.big_img\n",
    "tmp4 = torch.sum(mask_times_imgs[:how_many_to_show,chosen],dim=0).expand(ch_out,-1,-1)\n",
    "print(\"sum big_masks\", torch.max(tmp3))\n",
    "print(\"sum big_masks * big_imgs\", torch.max(tmp4))\n",
    "combined = torch.stack((tmp1,tmp2,tmp3,tmp4),dim=0)\n",
    "print(combined.shape)\n",
    "b = show_batch(combined, n_col=2, title=\"# ref, IMGS, MASKS, IMGS*MASKS\", figsize=(24,24))\n",
    "b.savefig(os.path.join(dir_output, \"ref_img_mask.png\"))\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.min(output.inference.big_mask[:how_many_to_show,chosen]), torch.max(output.inference.big_mask[:how_many_to_show,chosen]))\n",
    "show_batch(output.inference.big_mask[:how_many_to_show,chosen], n_col=4, title=\"# MASKS\", figsize=(24,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = show_batch(reference_imgs[chosen]+output.inference.big_mask[:how_many_to_show,chosen], \n",
    "               n_col=3, n_padding=4,title=\"# MASKS over REF, p=\"+str(prob_title), figsize=(24,24))\n",
    "b.savefig(os.path.join(dir_output, \"mask_over_ref.png\"))\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = show_batch(reference_imgs[chosen]+10*output.inference.big_img[:how_many_to_show,chosen], \n",
    "               n_col=4, n_padding=4,title=\"# IMGS over REF, p=\"+str(prob_title), figsize=(24,24), normalize_range=(0,1))\n",
    "b.savefig(os.path.join(dir_output, \"imgs_over_ref.png\"))\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.inference.prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob =  output.inference.prob[:,chosen, None, None, None]\n",
    "\n",
    "b_img = output.inference.big_img[:,chosen]\n",
    "ch_out = b_img.shape[-3]\n",
    "b_mask = output.inference.big_mask[:,chosen].expand(-1,ch_out,-1,-1)\n",
    "b_combined = b_img * b_mask * prob\n",
    "tmp = torch.cat((b_mask, b_img, b_combined), dim=0)\n",
    "b = show_batch(tmp, n_col=tmp.shape[0]//3, n_padding=4, title=\"# mask, imgs, product. p=\"+str(prob_title), figsize=(24,24))\n",
    "b.savefig(os.path.join(dir_output, \"mask_imgs_product.png\"))\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the probability map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(output.inference.p_map[chosen,0].cpu().numpy())\n",
    "_ = plt.colorbar()\n",
    "plt.savefig(os.path.join(dir_output, \"pmap_chosen.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE MOVIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=\"xxx\"\n",
    "a = show_batch(reference_imgs[:9],n_col=3,n_padding=4,title=\"REFERENCE\")\n",
    "b = show_batch(output.inference.p_map[:9],n_col=3,n_padding=4,title=\"EPOCH = \"+str(epoch))\n",
    "c = show_batch(output.inference.big_bg[:9],n_col=3,n_padding=4,title=\"EPOCH = \"+str(epoch))\n",
    "d = show_batch(output.imgs[:9],n_col=3,n_padding=4,title=\"EPOCH = \"+str(epoch))\n",
    "\n",
    "display(a,b,c,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# actual loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_output = '/home/jupyter/REPOS/spacetx-research/NEW_ARCHIVE/merfish_aug_5_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0,20000,5):\n",
    "    if(epoch<10):\n",
    "        label =\"0000\"+str(epoch)\n",
    "    elif(epoch<100):\n",
    "        label = \"000\"+str(epoch)\n",
    "    elif(epoch<1000):\n",
    "        label = \"00\"+str(epoch)\n",
    "    elif(epoch<10000):\n",
    "        label = \"0\"+str(epoch)\n",
    "    elif(epoch<100000):\n",
    "        label = str(epoch)\n",
    "    else:\n",
    "        raise Exception\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        ckpt_file = os.path.join(dir_output, \"ckp_\"+str(epoch)+\".pkl\")\n",
    "        resumed = file2resumed(path=ckpt_file, device=None)\n",
    "        load_model_optimizer(resumed=resumed, \n",
    "                             model=vae,\n",
    "                             optimizer=None,\n",
    "                             overwrite_member_var=True)\n",
    "        \n",
    "        print(\"epoch, label, prob_cor_factor ->\",epoch,label,vae.prob_corr_factor)\n",
    "        vae.train()\n",
    "        with torch.no_grad():\n",
    "            output = vae.forward(reference_imgs,\n",
    "                                 draw_image=True,\n",
    "                                 draw_boxes=True,\n",
    "                                 verbose=False)\n",
    "        \n",
    "        b=show_batch(output.imgs[:8],n_col=4,n_padding=4,title=\"EPOCH = \"+str(epoch))\n",
    "        b.savefig(os.path.join(dir_output, 'movie_rec_'+label+'.png'), bbox_inches='tight')\n",
    "        \n",
    "        b=show_batch(output.inference.p_map[:8],n_col=4,n_padding=4,title=\"EPOCH = \"+str(epoch), normalize_range=None)\n",
    "        b.savefig(os.path.join(dir_output, 'movie_map_'+label+'.png'), bbox_inches='tight') \n",
    "        \n",
    "        b=show_batch(output.inference.big_bg[:8],n_col=4,n_padding=4,title=\"EPOCH = \"+str(epoch))\n",
    "        b.savefig(os.path.join(dir_output, 'movie_bg_'+label+'.png'), bbox_inches='tight') \n",
    "        \n",
    "\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sorted list of image files so that I can create the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_filenames = glob.glob(dir_output+\"/movie_rec*.png\")\n",
    "map_filenames = glob.glob(dir_output+\"/movie_map*.png\")\n",
    "bg_filenames = glob.glob(dir_output+\"/movie_bg*.png\")\n",
    "\n",
    "rec_filenames.sort()\n",
    "map_filenames.sort()\n",
    "bg_filenames.sort()\n",
    "print(rec_filenames)\n",
    "print(map_filenames)\n",
    "print(bg_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_frame_rec(n):\n",
    "    return Image(filename=rec_filenames[n])\n",
    "\n",
    "def show_frame_map(n):\n",
    "    return Image(filename=map_filenames[n])\n",
    "\n",
    "def show_frame_bg(n):\n",
    "    return display.Image(filename=bg_filenames[n])\n",
    "\n",
    "def show_frame_all(n):\n",
    "    try:\n",
    "        a = Image(filename=bg_filenames[n])\n",
    "        b = Image(filename=map_filenames[n])\n",
    "        c = Image(filename=rec_filenames[n])\n",
    "        return display(a,b,c)\n",
    "    except IndexError:\n",
    "        print(\"list index out of range\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a gif file\n",
    "import moviepy.editor as mpy\n",
    "\n",
    "movie_rec = os.path.join(dir_output, \"movie_rec.gif\")\n",
    "movie_map = os.path.join(dir_output, \"movie_map.gif\")\n",
    "movie_bg = os.path.join(dir_output, \"movie_bg.gif\")\n",
    "\n",
    "frame_per_second = 2\n",
    "im = mpy.ImageSequenceClip(rec_filenames, fps=frame_per_second)\n",
    "im.write_gif(movie_rec, fps=frame_per_second)\n",
    "\n",
    "im = mpy.ImageSequenceClip(map_filenames, fps=frame_per_second)\n",
    "im.write_gif(movie_map, fps=frame_per_second)\n",
    "\n",
    "im = mpy.ImageSequenceClip(bg_filenames, fps=frame_per_second)\n",
    "im.write_gif(movie_bg, fps=frame_per_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(reference_imgs[:8],n_col=4,n_padding=4,title=\"REFERENCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"<img src=\"+movie_rec+\"></img>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"<img src=\"+movie_map+\"></img>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(reference_imgs[:8],n_col=4,n_padding=4,title=\"REFERENCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"<img src=\"+movie_bg+\"></img>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at few frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame_all(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame_all(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame_all(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame_all(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame_all(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(reference_imgs[:8],n_col=4,n_padding=4,title=\"REFERENCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame_all(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame_all(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame_all(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame_all(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame_all(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame_all(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame_all(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame_all(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(reference_imgs[:8],n_col=4,n_padding=4,title=\"REFERENCE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL CHECK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_in_tmp, labels, index = train_loader.load(batch_size=8)\n",
    "auch = vae.generate(imgs_in=imgs_in_tmp[:1], draw_boxes=False)\n",
    "\n",
    "pmap_gen = show_batch(auch.inference.p_map[:8], title=\"generated p_map\")\n",
    "imgs_gen = show_batch(auch.imgs[:8], title=\"generated imgs\")\n",
    "display(pmap_gen, imgs_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_mask = auch.inference.big_mask[:,0]\n",
    "big_img = auch.inference.big_img[:,0]\n",
    "tmp = torch.cat((big_mask, big_img),dim=0)\n",
    "print(auch.inference.prob)\n",
    "show_batch(tmp, n_col=tmp.shape[0]//2, title=\"masks and imgs\", figsize=(24,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auch.inference.big_mask.shape)\n",
    "fg_mask = torch.sum(auch.inference.big_mask, dim=0)\n",
    "img = torch.sum(auch.inference.big_img, dim=0)\n",
    "print(fg_mask.shape)\n",
    "\n",
    "figure, axes = plt.subplots(ncols=3, figsize=(24, 24))\n",
    "axes[0].imshow(fg_mask[0,0].cpu(), cmap='gray')\n",
    "axes[1].imshow(img[0,0].cpu(), cmap='gray')\n",
    "axes[2].imshow((fg_mask[0,0]*img[0,0]).cpu(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
