{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composite variational auto-encoder\n",
    "\n",
    "This notebook implements the composite variational auto-encoder.\n",
    "\n",
    "At the moment, only the generator is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.functional import grid_sample, affine_grid\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam, Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)\n",
    "smoke_test = 'CI' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeVAEModelParams(dict):\n",
    "    def __init__(self):\n",
    "        super(CompositeVAEModelParams, self).__init__()\n",
    "        \n",
    "        # parameters for BasicObjectDecoder\n",
    "        self['basic_object_decoder.width'] = 64\n",
    "        self['basic_object_decoder.height'] = 64\n",
    "        self['basic_object_decoder.z_dim'] = 64\n",
    "        self['basic_object_decoder.hidden_dim'] = 1024\n",
    "        \n",
    "        # parameters for the object grid\n",
    "        self['object_grid.width'] = 8\n",
    "        self['object_grid.height'] = 6\n",
    "        \n",
    "        # parameters of the dataset\n",
    "        self['observed_image.width'] = 384\n",
    "        self['observed_image.height'] = 256\n",
    "        \n",
    "        # parameters for CompositeVAE\n",
    "        self['composite_vae.active_cell_prob'] = 0.5\n",
    "        self['composite_vae.dx_scale'] = 0.2\n",
    "        self['composite_vae.dy_scale'] = 0.2\n",
    "        self['composite_vae.s_loc'] = 1.0\n",
    "        self['composite_vae.s_scale'] = 0.1\n",
    "        \n",
    "\n",
    "class BasicObjectDecoder(nn.Module):\n",
    "    \"\"\"This module decodes the latent representation of a single object and decodes it into an image.\"\"\"\n",
    "    def __init__(self, params: dict):\n",
    "        super(BasicObjectDecoder, self).__init__()\n",
    "        \n",
    "        self.width = params['basic_object_decoder.width']\n",
    "        self.height = params['basic_object_decoder.height']\n",
    "        self.z_dim = params['basic_object_decoder.z_dim']\n",
    "        self.hidden_dim = params['basic_object_decoder.hidden_dim']\n",
    "        \n",
    "        self.pixel_count = self.width * self.height\n",
    "        self.fc1 = nn.Linear(self.z_dim, self.hidden_dim)\n",
    "        self.fc21 = nn.Linear(self.hidden_dim, self.pixel_count)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z_obj):\n",
    "        hidden = self.relu(self.fc1(z_obj))\n",
    "        pixel_intensity = self.sigmoid(self.fc21(hidden))\n",
    "        return pixel_intensity.reshape(z_obj.shape[:-1] + (self.width, self.height))\n",
    "    \n",
    "    def load_params(self, decoder_state_dict):\n",
    "        self.load_state_dict(decoder_state_dict)\n",
    "\n",
    "        \n",
    "class BasicImageTransformer(nn.Module):\n",
    "    \"\"\"This module transforms a batch of object images into their final location in the\n",
    "    to-be-composed image (see `forward` below for more details).\n",
    "    \"\"\"\n",
    "    def __init__(self, params: dict):\n",
    "        super(BasicImageTransformer, self).__init__()\n",
    "        self.params = params\n",
    "        \n",
    "        delta_x = 2. / params['object_grid.width']\n",
    "        delta_y = 2. / params['object_grid.height']\n",
    "        \n",
    "        self.grid_offset_x, self.grid_offset_y = torch.meshgrid((\n",
    "            -1. + 0.5 * delta_x + delta_x * torch.arange(0, params['object_grid.width']).float(),\n",
    "            -1. + 0.5 * delta_y + delta_y * torch.arange(0, params['object_grid.height']).float()))\n",
    "        \n",
    "    @classmethod\n",
    "    def generate_stn_theta(cls, tx, ty, sx, sy):\n",
    "        \"\"\"Generates STN matrix from explicit scales and translations.\n",
    "        \n",
    "        Here, `tx`, `ty`, `sx`, and `sy` refer to $\\theta_{13}$, $\\theta_{23}$, $\\theta_{11}$,\n",
    "        and $\\theta_{22}$ in Eq. (1) in Ref. [1].\n",
    "        \n",
    "        Note:\n",
    "            All of the dimensions of `tx`, `ty`, `sx`, and `sy` are treated as batch dimensions,\n",
    "            and all tensors must have the same shape. Crucially, these tensors must not have unsqueezed\n",
    "            flanking singleton dimensions on the right.\n",
    "            \n",
    "        Returns:\n",
    "            A tensor with shape (..., 2, 3) corresponding to the affine transformation to be ingested\n",
    "            by `torch.nn.functional.affine_grid`.\n",
    "            \n",
    "        References:\n",
    "            [1] Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. \"Spatial transformer networks.\"\n",
    "            Advances in Neural Information Processing Systems, pp. 2017-2025. 2015.\n",
    "        \"\"\"\n",
    "        batch_shape = sx.shape\n",
    "        combined = torch.cat(\n",
    "            (tx.view(*batch_shape, 1), # 0\n",
    "             ty.view(*batch_shape, 1), # 1\n",
    "             sx.view(*batch_shape, 1), # 2\n",
    "             sy.view(*batch_shape, 1), # 3\n",
    "             torch.zeros(*batch_shape, 1)), -1) # 4\n",
    "        expansion_indices = torch.LongTensor([3, 4, 1, 4, 2, 0])\n",
    "        return torch.index_select(combined, -1, expansion_indices).view(*batch_shape, 2, 3)\n",
    "\n",
    "    @classmethod\n",
    "    def transform_objects(cls, obj_images, obj_tx, obj_ty, obj_sx, obj_sy, output_width, output_height):\n",
    "        \"\"\"Applies an affine transformation on `obj_images` as specified by `obj_tx`, `obj_ty`, `obj_sx`,\n",
    "        and `obj_sy`.\n",
    "        \n",
    "        Arguments:\n",
    "            obj_images: A tensor with shape (..., Ws, Hs) where Ws and Hs corrspond to the horizontal\n",
    "                and vertical source dimensions, respectively.\n",
    "            obj_tx: A tensor with shape (...) (see `BasicImageTransformer.generate_stn_theta`)\n",
    "            obj_ty: A tensor with shape (...) (see `BasicImageTransformer.generate_stn_theta`)\n",
    "            obj_sx: A tensor with shape (...) (see `BasicImageTransformer.generate_stn_theta`)\n",
    "            obj_sy: A tensor with shape (...) (see `BasicImageTransformer.generate_stn_theta`)\n",
    "            output_width: (positive integer) output width\n",
    "            output_height: (positive integer) output height\n",
    "        \n",
    "        Returns:\n",
    "            A tensor with shape (..., output_width, output_height) containing the spatially\n",
    "            transformed images.\n",
    "        \"\"\"\n",
    "        obj_width = obj_images.size(-2)\n",
    "        obj_height = obj_images.size(-1)\n",
    "        theta = cls.generate_stn_theta(obj_tx, obj_ty, obj_sx, obj_sy)\n",
    "        theta_flat = theta.view(-1, 2, 3)\n",
    "        obj_images_flat = obj_images.view(-1, obj_width, obj_height)\n",
    "        n_objects_total = obj_images_flat.size(0)\n",
    "        grid = affine_grid(theta_flat, torch.Size((n_objects_total, 1, output_width, output_height)))\n",
    "        out = grid_sample(obj_images_flat.view(n_objects_total, 1, obj_width, obj_height), grid)\n",
    "        return out.view(obj_images.shape[:-2] + (output_width, output_height))\n",
    "\n",
    "    def forward(self, obj_images, obj_dx, obj_dy, obj_s):\n",
    "        \"\"\"Transforms a batch of object images into their final spatial position.\n",
    "        \n",
    "        Arguments:\n",
    "            obj_images: A tensor with the following shape,              \n",
    "                    \n",
    "                    (...,\n",
    "                     object_grid.width,\n",
    "                     object_grid.height,\n",
    "                     basic_object_decoder.width,\n",
    "                     basic_object_decoder.height),\n",
    "                \n",
    "                containing images of individual object in the order deemed to be placed on the target\n",
    "                grid. Here, ... denotes extra batch dimensions.\n",
    "                \n",
    "            obj_dx: A tensor with shape (..., object_grid.width, object_grid.height), denoting\n",
    "                the horizontal centroid-to-centroid displacement of each object image with respect\n",
    "                to the target grid cell. For example, dx = -1 (+1) implies aligning the centroid of the\n",
    "                object to the left (right) edge of the cell.\n",
    "                \n",
    "            obj_dy: A tensor with shape (..., object_grid.width, object_grid.height), denoting\n",
    "                the vertical centroid-to-centroid displacement of each object image with respect\n",
    "                to the target grid cell. For example, dy = -1 (+1) implies aligning the centroid of the\n",
    "                object to the bottom (top) edge of the cell.\n",
    "                \n",
    "            obj_s: A (positive) tensor with shape (..., object_grid.width, object_grid.height), denoting\n",
    "                the relative scale of each object image with respect to the target grid cell dimensions.\n",
    "                For example, s = 1 implies fitting the object image to the target grid cell whereas s = 0.5\n",
    "                implies shrinking the object in half and then embedding inside the target grid cell.\n",
    "                \n",
    "        Returns:\n",
    "            A tensor with the following shape:\n",
    "            \n",
    "                (...,\n",
    "                     object_grid.width,\n",
    "                     object_grid.height,\n",
    "                     observed_image.width,\n",
    "                     observed_image.height).\n",
    "                     \n",
    "            Crucially, the output is differentible with respect to all of the input arguments.\n",
    "        \"\"\"\n",
    "        batch_shape = obj_images.shape[:-2]\n",
    "        grid_width = self.params['object_grid.width']\n",
    "        grid_height = self.params['object_grid.height']\n",
    "        \n",
    "        # generate STN transformation parameters\n",
    "        #\n",
    "        # Note:\n",
    "        #\n",
    "        #   These expressions are obtained by inverting the affine transformation\n",
    "        #   (see the Mathematica notebook `affine_trans_eq.nb` for details.)\n",
    "        #\n",
    "        obj_tx = -(grid_width * self.grid_offset_x + 2 * obj_dx) / obj_s\n",
    "        obj_ty = -(grid_height * self.grid_offset_y + 2 * obj_dy) / obj_s\n",
    "        obj_sx = float(grid_width) / obj_s\n",
    "        obj_sy = float(grid_height) / obj_s\n",
    "        \n",
    "        # transform to the main image\n",
    "        trasformed_obj_images = self.transform_objects(\n",
    "            obj_images, obj_tx, obj_ty, obj_sx, obj_sy,\n",
    "            self.params['observed_image.width'],\n",
    "            self.params['observed_image.height'])\n",
    "                \n",
    "        return trasformed_obj_images\n",
    "    \n",
    "\n",
    "class BasicImageComposer(nn.Module):\n",
    "    \"\"\"This module takes spatially transformed objects and composes the final image.\"\"\"\n",
    "    def __init__(self, params: dict):\n",
    "        super(BasicImageComposer, self).__init__()\n",
    "        self.params = params\n",
    "        \n",
    "    def forward(self, trasformed_obj_images, obj_active):\n",
    "        \"\"\"Compose the final image by simpling adding the image of spatially transformed object,\n",
    "        and applying a sigmoidal transformation to ensure that the intensities are < 1.0.\n",
    "        \n",
    "        Arguments:\n",
    "            trasformed_obj_images: A tensor with the following shape,\n",
    "            \n",
    "                    (...,\n",
    "                         object_grid.width,\n",
    "                         object_grid.height,\n",
    "                         observed_image.width,\n",
    "                         observed_image.height),\n",
    "                     \n",
    "                that contains spatially transformed images of indivual objects (e.g. the output\n",
    "                of `BasicImageTransformer.forward`.)\n",
    "\n",
    "            obj_active: A tensor with shape (..., object_grid.width, object_grid.height) that\n",
    "                implies the presence or absence of an object in the final image.\n",
    "            \n",
    "        Returns:\n",
    "            A tensor with shape (..., observed_image.width, observed_image.height) corresponding\n",
    "            to a fully-composed image.\n",
    "        \"\"\"\n",
    "        \n",
    "        added_intensities = torch.sum(\n",
    "            torch.sum(\n",
    "                trasformed_obj_images * obj_active.view(*obj_active.shape, 1, 1),\n",
    "                dim=-3),\n",
    "            dim=-3)\n",
    "        \n",
    "        return torch.sigmoid(added_intensities)\n",
    "    \n",
    "\n",
    "class CompositeVAE(nn.Module):\n",
    "    def __init__(self, params: dict):\n",
    "        super(CompositeVAE, self).__init__()\n",
    "        self.basic_object_decoder = BasicObjectDecoder(params)\n",
    "        self.transformer = BasicImageTransformer(params) \n",
    "        self.composer = BasicImageComposer(params) \n",
    "        self.params = params\n",
    "    \n",
    "    def model(self, images: torch.Tensor):\n",
    "        \"\"\"Specifiation of the CompositeVAE model.\n",
    "        \n",
    "        Arguments:\n",
    "            images: a tensor of shape (n_batch, observed_images.width, observed_images.height)\n",
    "                where n_batch >= 1 is allowed to vary.\n",
    "        \"\"\"\n",
    "        \n",
    "        n_batch = images.size(0)\n",
    "        batch_grid_shape = [n_batch, self.params['object_grid.width'], self.params['object_grid.height']]\n",
    "        \n",
    "        with pyro.iarange(\"images\", dim=-1, size=n_batch):    \n",
    "            # draw a Bernoulli for each grid point (cell presence/absence)\n",
    "            obj_active = pyro.sample(\"obj_active\",\n",
    "                dist.Bernoulli(probs=self.params['composite_vae.active_cell_prob'])\n",
    "                    .expand_by(batch_grid_shape))\n",
    "\n",
    "            # draw basic object latent codes for each grid point\n",
    "            obj_z_loc = torch.zeros(self.params['basic_object_decoder.z_dim'])\n",
    "            obj_z_scale = torch.ones(self.params['basic_object_decoder.z_dim'])\n",
    "            obj_z = pyro.sample(\"obj_z\",\n",
    "                dist.Normal(obj_z_loc, obj_z_scale)\n",
    "                    .expand_by(batch_grid_shape)\n",
    "                    .independent(3))\n",
    "            \n",
    "            # decode the latent code of basic objects into images\n",
    "            obj_images = self.basic_object_decoder(obj_z)\n",
    "            \n",
    "            # displacement and scale of basic objects\n",
    "            obj_dx = pyro.sample(\"obj_dx\",\n",
    "                dist.Normal(0, self.params['composite_vae.dx_scale'])\n",
    "                    .expand_by(batch_grid_shape)\n",
    "                    .independent(3))\n",
    "            obj_dy = pyro.sample(\"obj_dy\",\n",
    "                dist.Normal(0, self.params['composite_vae.dy_scale'])\n",
    "                    .expand_by(batch_grid_shape)\n",
    "                    .independent(3))\n",
    "            \n",
    "            alpha =  self.params['composite_vae.s_loc']**2 / self.params['composite_vae.s_scale']**2\n",
    "            beta = self.params['composite_vae.s_loc'] / self.params['composite_vae.s_scale']**2\n",
    "            obj_s = pyro.sample(\"obj_s\",\n",
    "                dist.Gamma(concentration=alpha, rate=beta)\n",
    "                    .expand_by(batch_grid_shape)\n",
    "                    .independent(3))\n",
    "            \n",
    "            # transform to the observed image frame\n",
    "            transform_objects = self.transformer.forward(obj_images, obj_dx, obj_dy, obj_s)\n",
    "            \n",
    "            # compose to the observed image frame\n",
    "            composed_images = self.composer.forward(transform_objects, obj_active)\n",
    "            \n",
    "        return composed_images\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = CompositeVAEModelParams()\n",
    "cvae = CompositeVAE(params)\n",
    "\n",
    "# initialize the decoder with the VAE we trained earlier on synthetic DAPI stains\n",
    "cvae.basic_object_decoder.load_params(torch.load('./synth_dapi_vae_params/dapi_vae_decoder_params.pt'))\n",
    "\n",
    "composed_images = cvae.model(torch.zeros(1, 1, 1))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "img = composed_images.detach().numpy()[0, :, :].T\n",
    "plt.imshow(img, origin='lower', cmap=plt.cm.Greys_r)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
