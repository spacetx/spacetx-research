{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAPI stain variational auto-encoder\n",
    "\n",
    "This notebook trains a variational auto-encoder (VAE) from DAPI stains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam, Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)\n",
    "smoke_test = 'CI' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_loaders(batch_size=128, use_cuda=False):\n",
    "    train_dataset_npy = './synth_dapi_stains/synth_dapi_stains_train_64_64_10k.npy'\n",
    "    test_dataset_npy = './synth_dapi_stains/synth_dapi_stains_test_64_64_1k.npy'\n",
    "    \n",
    "    train_set = TensorDataset(torch.Tensor(np.load(train_dataset_npy)))\n",
    "    test_set = TensorDataset(torch.Tensor(np.load(test_dataset_npy)))\n",
    "    \n",
    "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "        batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"A basic two layer dense encoder.\"\"\"\n",
    "    def __init__(self, width, height, z_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.pixel_count = width * height\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.pixel_count, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, self.pixel_count)\n",
    "        hidden = self.relu(self.fc1(x))\n",
    "        z_loc = self.fc21(hidden)\n",
    "        z_scale = torch.exp(self.fc22(hidden))\n",
    "        return z_loc, z_scale\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"A basic two layer dense decoder.\"\"\"\n",
    "    def __init__(self, width, height, z_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.pixel_count = width * height\n",
    "        \n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, self.pixel_count)\n",
    "\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z):\n",
    "        hidden = self.relu(self.fc1(z))\n",
    "        loc_img = self.sigmoid(self.fc21(hidden))\n",
    "        return loc_img\n",
    "\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, width, height, z_dim=50, hidden_dim=400, use_cuda=False):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.pixel_count = width * height\n",
    "\n",
    "        self.encoder = Encoder(width, height, z_dim, hidden_dim)\n",
    "        self.decoder = Decoder(width, height, z_dim, hidden_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def model(self, x):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        with pyro.iarange(\"data\", batch_size):\n",
    "            z_loc = x.new_zeros(batch_size, self.z_dim)\n",
    "            z_scale = x.new_ones(batch_size, self.z_dim)\n",
    "            \n",
    "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).independent(1))\n",
    "            \n",
    "            pixel_loc = self.decoder.forward(z)\n",
    "            pixel_scale = 0.1 * torch.ones_like(pixel_loc)\n",
    "            \n",
    "            # TODO: consider a better observation distribution\n",
    "            pyro.sample(\"obs\",\n",
    "                        dist.Normal(pixel_loc, pixel_scale).independent(1),\n",
    "                        obs=x.reshape(-1, self.pixel_count))\n",
    "\n",
    "    def guide(self, x):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        with pyro.iarange(\"data\", batch_size):\n",
    "            z_loc, z_scale = self.encoder.forward(x)\n",
    "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).independent(1))\n",
    "\n",
    "    def reconstruct_img(self, x):\n",
    "        z_loc, z_scale = self.encoder(x)\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        loc_img = self.decoder(z)\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=False):\n",
    "    epoch_loss = 0.\n",
    "    for _, (x,) in enumerate(train_loader):\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        epoch_loss += svi.step(x)\n",
    "\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    \n",
    "    return total_epoch_loss_train\n",
    "\n",
    "\n",
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "    test_loss = 0.\n",
    "    for i, (x,) in enumerate(test_loader):\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        test_loss += svi.evaluate_loss(x)\n",
    "\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    \n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "USE_CUDA = False\n",
    "Z_DIM = 64\n",
    "HIDDEN_DIM = 1024\n",
    "WIDTH = 64\n",
    "HEIGHT = 64\n",
    "\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS = 1 if smoke_test else 300\n",
    "TEST_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = setup_data_loaders(batch_size=256, use_cuda=USE_CUDA)\n",
    "\n",
    "# clear pyro params\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup the VAE\n",
    "vae = VAE(use_cuda=USE_CUDA, z_dim=Z_DIM, hidden_dim=HIDDEN_DIM, width=WIDTH, height=HEIGHT)\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": LEARNING_RATE}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model parameters (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.encoder.load_state_dict(torch.load('./synth_dapi_vae_params/dapi_vae_encoder_params.pt'))\n",
    "vae.decoder.load_state_dict(torch.load('./synth_dapi_vae_params/dapi_vae_decoder_params.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model (can skip if parameters loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, test_loader, use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction of a real spot\n",
    "(real_img,) = test_loader.dataset[28]\n",
    "plt.figure()\n",
    "plt.imshow(real_img.numpy().squeeze(), cmap=plt.cm.Greys_r)\n",
    "\n",
    "plt.figure()\n",
    "rec_img = vae.reconstruct_img(real_img).reshape_as(real_img)\n",
    "plt.imshow(rec_img.detach().numpy().squeeze(), cmap=plt.cm.Greys_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interactive, FloatSlider\n",
    "\n",
    "z_rand = torch.Tensor(np.random.randn(Z_DIM))\n",
    "dz_dir_1 = np.random.randn(Z_DIM)\n",
    "dz_dir_2 = np.random.randn(Z_DIM)\n",
    "dz_dir_3 = np.random.randn(Z_DIM)\n",
    "\n",
    "def plot_dapi_stain(dz_1, dz_2, dz_3):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    with torch.no_grad():\n",
    "        z_new = z_rand.numpy() + dz_1 * dz_dir_1 + dz_2 * dz_dir_2 + dz_3 * dz_dir_3 \n",
    "        img = vae.decoder.forward(torch.Tensor(z_new)).reshape(WIDTH, HEIGHT).numpy()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, cmap=plt.cm.Greys_r)\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(plot_dapi_stain,\n",
    "                               dz_1=FloatSlider(min=-1., max=1., step=0.1),\n",
    "                               dz_2=FloatSlider(min=-1., max=1., step=0.1),\n",
    "                               dz_3=FloatSlider(min=-1., max=1., step=0.1))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.encoder.state_dict(), './synth_dapi_vae_params/dapi_vae_encoder_params.pt')\n",
    "torch.save(vae.decoder.state_dict(), './synth_dapi_vae_params/dapi_vae_decoder_params.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
