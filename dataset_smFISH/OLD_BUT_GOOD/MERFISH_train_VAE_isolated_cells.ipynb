{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN VAE ON ISOLATED CELLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy\n",
    "from MODULES.utilities import *\n",
    "from MODULES.vae_model import * \n",
    "\n",
    "#pip install psutil\n",
    "#pip install neptune-client\n",
    "#pip install neptune-notebooks\n",
    "#!jupyter nbextension enable --py neptune-notebooks\n",
    "\n",
    "#conda install -c conda-forge nodejs\n",
    "#!jupyter labextension install neptune-notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "def log_model_summary(experiment, model):\n",
    "    for x in model.__str__().split('\\n'):\n",
    "        # replace leading spaces with '-' character\n",
    "        n = len(x) - len(x.lstrip(' '))\n",
    "        experiment.log_text(\"model summary\", '-'*n + x)\n",
    "        \n",
    "def log_metrics(experiment, metric_tuple, prefix: str = \"\"):\n",
    "    for key in metric_tuple._fields:\n",
    "        value = getattr(metric_tuple, key).item()\n",
    "        if isinstance(value,float):\n",
    "            experiment.log_metric(prefix+key,value)\n",
    "            \n",
    "def replace_artifact(experiment, file_list: list, delay: int=5):\n",
    "    assert delay >= 1\n",
    "    assert len(file_list) >= 1\n",
    "    \n",
    "    if len(file_list) <= delay:\n",
    "        experiment.log_artifact(file_list[-1])\n",
    "    else:\n",
    "        # i.e. len(file_list) > delay:\n",
    "        experiment.log_artifact(file_list[-1])\n",
    "        experiment.delete_artifacts(file_list[-1-delay])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NVMLError: NVML Shared Library Not Found - GPU usage metrics may not be reported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/dalessioluca/sandbox-2/e/SAN2-25\n"
     ]
    }
   ],
   "source": [
    "neptune.set_project('dalessioluca/sandbox-2')\n",
    "\n",
    "params = load_json_as_dict(\"./parameters_smFISH_factor8.json\")\n",
    "\n",
    "exp = neptune.create_experiment(params=flatten_dict(params),\n",
    "                                upload_source_files=[\"./MODULES/vae_model.py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_stuff = load_obj(\"./isolated_cells_dataset.pt\")\n",
    "#exp.set_property(\"dataset\", hashlib.md5(small_stuff).hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 80\n",
    "N_TRAIN = int(0.8 * small_stuff.shape[0])\n",
    " \n",
    "train_loader = SpecialDataSet(img=small_stuff[:N_TRAIN], \n",
    "                              store_in_cuda=torch.cuda.is_available(),\n",
    "                              shuffle=True,\n",
    "                              drop_last=True,\n",
    "                              batch_size=BATCH_SIZE)\n",
    "\n",
    "test_loader = SpecialDataSet(img=small_stuff[N_TRAIN:], \n",
    "                             store_in_cuda=torch.cuda.is_available(),\n",
    "                             shuffle=False,\n",
    "                             drop_last=True,\n",
    "                             batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset lenght: 1047\n",
      "img.shape torch.Size([1047, 1, 28, 28])\n",
      "img.dtype torch.float32\n",
      "img.device cpu\n",
      "MINIBATCH: img.shapes labels.shape, index.shape -> torch.Size([8, 1, 28, 28]) torch.Size([8]) torch.Size([8])\n",
      "MINIBATCH: min and max of minibatch tensor(0.0301) tensor(0.5948)\n"
     ]
    }
   ],
   "source": [
    "train_batch_example = train_loader.check_batch()\n",
    "exp.log_image(\"train_batch_example\", train_batch_example)\n",
    "#train_batch_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset lenght: 262\n",
      "img.shape torch.Size([262, 1, 28, 28])\n",
      "img.dtype torch.float32\n",
      "img.device cpu\n",
      "MINIBATCH: img.shapes labels.shape, index.shape -> torch.Size([8, 1, 28, 28]) torch.Size([8]) torch.Size([8])\n",
      "MINIBATCH: min and max of minibatch tensor(0.0301) tensor(0.5878)\n"
     ]
    }
   ],
   "source": [
    "test_batch_example = test_loader.check_batch()\n",
    "exp.log_image(\"test_batch_example\", test_batch_example)\n",
    "#test_batch_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_imgs, _, _ = test_loader.load(8)\n",
    "reference = show_batch(reference_imgs, n_padding=4, figsize=(12,12))\n",
    "exp.log_image(\"reference\", reference)\n",
    "#reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = SimpleVae(params)\n",
    "log_model_summary(experiment=exp, model=vae)\n",
    "\n",
    "history_dict = {}\n",
    "min_test_loss = 9999999999\n",
    "dir_output = \"./\"\n",
    "\n",
    "optimizer = instantiate_optimizer(model=vae, dict_params_optimizer=params[\"optimizer\"])\n",
    "\n",
    "if params[\"optimizer\"][\"scheduler_is_active\"]:\n",
    "    scheduler = instantiate_scheduler(optimizer=optimizer, dict_params_scheduler=params[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =   0 train_loss=36.51169\n",
      "i =   1 train_loss=32.89920\n",
      "i =   2 train_loss=29.78791\n",
      "i =   3 train_loss=24.30220\n",
      "i =   4 train_loss=15.40609\n",
      "i =   5 train_loss=11.61520\n",
      "i =   6 train_loss=12.15938\n",
      "i =   7 train_loss=11.71213\n",
      "i =   8 train_loss=9.09930\n",
      "i =   9 train_loss=6.23104\n",
      "i =  10 train_loss=4.78668\n",
      "i =  11 train_loss=4.97643\n",
      "i =  12 train_loss=4.90802\n",
      "Train [epoch    0] loss=15.723, mse=19.110, kl_tot=1.923, geco_bal=0.805\n",
      "i =   0 train_loss=4.00009\n",
      "i =   1 train_loss=4.21980\n",
      "i =   2 train_loss=4.61480\n",
      "Test  [epoch    0] loss=4.278, mse=5.079, kl_tot=0.849, geco_bal=0.811\n",
      "saved files -> ./ckp_20.pkl  ./history_20.pkl\n",
      "Train [epoch    1] loss=3.414, mse=3.864, kl_tot=1.455, geco_bal=0.814\n",
      "Train [epoch    2] loss=2.615, mse=3.043, kl_tot=0.699, geco_bal=0.818\n",
      "Train [epoch    3] loss=2.361, mse=2.726, kl_tot=0.695, geco_bal=0.820\n",
      "Train [epoch    4] loss=2.205, mse=2.521, kl_tot=0.742, geco_bal=0.823\n",
      "Test  [epoch    4] loss=2.025, mse=2.291, kl_tot=0.782, geco_bal=0.824\n",
      "saved files -> ./ckp_20.pkl  ./history_20.pkl\n",
      "Train [epoch    5] loss=2.126, mse=2.389, kl_tot=0.889, geco_bal=0.825\n",
      "Train [epoch    6] loss=2.017, mse=2.207, kl_tot=1.112, geco_bal=0.827\n",
      "Train [epoch    7] loss=1.930, mse=2.099, kl_tot=1.116, geco_bal=0.829\n",
      "Train [epoch    8] loss=1.788, mse=1.937, kl_tot=1.064, geco_bal=0.830\n",
      "Test  [epoch    8] loss=1.570, mse=1.671, kl_tot=1.076, geco_bal=0.831\n",
      "saved files -> ./ckp_20.pkl  ./history_20.pkl\n",
      "Train [epoch    9] loss=1.630, mse=1.752, kl_tot=1.025, geco_bal=0.832\n",
      "Train [epoch   10] loss=1.474, mse=1.572, kl_tot=0.984, geco_bal=0.833\n",
      "Train [epoch   11] loss=1.215, mse=1.224, kl_tot=1.169, geco_bal=0.835\n",
      "Train [epoch   12] loss=1.049, mse=1.019, kl_tot=1.202, geco_bal=0.835\n",
      "Test  [epoch   12] loss=0.971, mse=0.912, kl_tot=1.267, geco_bal=0.835\n",
      "saved files -> ./ckp_20.pkl  ./history_20.pkl\n",
      "Train [epoch   13] loss=0.976, mse=0.924, kl_tot=1.241, geco_bal=0.835\n",
      "Train [epoch   14] loss=0.953, mse=0.889, kl_tot=1.279, geco_bal=0.836\n",
      "Train [epoch   15] loss=0.952, mse=0.901, kl_tot=1.214, geco_bal=0.836\n",
      "Train [epoch   16] loss=0.922, mse=0.856, kl_tot=1.259, geco_bal=0.836\n",
      "Test  [epoch   16] loss=0.874, mse=0.809, kl_tot=1.200, geco_bal=0.836\n",
      "saved files -> ./ckp_20.pkl  ./history_20.pkl\n",
      "Train [epoch   17] loss=0.894, mse=0.830, kl_tot=1.220, geco_bal=0.836\n",
      "Train [epoch   18] loss=0.868, mse=0.788, kl_tot=1.272, geco_bal=0.836\n",
      "Train [epoch   19] loss=0.837, mse=0.754, kl_tot=1.256, geco_bal=0.836\n",
      "Train [epoch   20] loss=0.812, mse=0.719, kl_tot=1.279, geco_bal=0.835\n",
      "Test  [epoch   20] loss=0.759, mse=0.663, kl_tot=1.248, geco_bal=0.835\n",
      "saved files -> ./ckp_20.pkl  ./history_20.pkl\n",
      "Train [epoch   21] loss=0.776, mse=0.671, kl_tot=1.307, geco_bal=0.835\n",
      "Train [epoch   22] loss=0.747, mse=0.639, kl_tot=1.294, geco_bal=0.835\n",
      "Train [epoch   23] loss=0.727, mse=0.611, kl_tot=1.315, geco_bal=0.835\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1ae23a7cce5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             train_metrics = process_one_epoch(model=vae, \n\u001b[0m\u001b[1;32m     15\u001b[0m                                               \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DAPI_unsupervised/spacetx-research/DATASET_smFISH/MODULES/utilities.py\u001b[0m in \u001b[0;36mprocess_one_epoch\u001b[0;34m(model, dataloader, optimizer, weight_clipper, verbose)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# do back_prop and compute all the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyro/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyro/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TEST_FREQUENCY = 4 #params[\"simulation\"][\"TEST_FREQUENCY\"]\n",
    "CHECKPOINT_FREQUENCY = params[\"simulation\"][\"CHECKPOINT_FREQUENCY\"]\n",
    "NUM_EPOCHS = params[\"simulation\"][\"MAX_EPOCHS\"]\n",
    "epoch_restart = -1\n",
    "checkpoint_files, history_files = [], []\n",
    "\n",
    "for delta_epoch in range(1,NUM_EPOCHS+1):\n",
    "    epoch = delta_epoch+epoch_restart    \n",
    "        \n",
    "    #with torch.autograd.set_detect_anomaly(True):\n",
    "    with torch.autograd.set_detect_anomaly(False):\n",
    "        with torch.enable_grad():\n",
    "            vae.train()\n",
    "            train_metrics = process_one_epoch(model=vae, \n",
    "                                              dataloader=train_loader, \n",
    "                                              optimizer=optimizer, \n",
    "                                              verbose=(epoch==0), \n",
    "                                              weight_clipper=None)\n",
    "            \n",
    "            \n",
    "        with torch.no_grad():      \n",
    "            print(\"Train \"+train_metrics.pretty_print(epoch))\n",
    "            log_metrics(exp, train_metrics, prefix=\"train_\")\n",
    "            \n",
    "            history_dict = append_tuple_to_dict(source_tuple=train_metrics, \n",
    "                                               target_dict=history_dict,\n",
    "                                               prefix_exclude=\"wrong_examples\",\n",
    "                                               prefix_to_add=\"train_\")\n",
    "        \n",
    "    if params[\"optimizer\"][\"scheduler_is_active\"]:\n",
    "        scheduler.step()\n",
    "    \n",
    "    if(epoch % TEST_FREQUENCY == 0):\n",
    "        with torch.no_grad():\n",
    "            vae.eval()\n",
    "            test_metrics = process_one_epoch(model=vae, \n",
    "                                             dataloader=test_loader, \n",
    "                                             optimizer=optimizer, \n",
    "                                             verbose=(epoch==0), \n",
    "                                             weight_clipper=None)\n",
    "            print(\"Test  \"+test_metrics.pretty_print(epoch))\n",
    "            log_metrics(exp, train_metrics, prefix=\"test_\")\n",
    "        \n",
    "            history_dict = append_tuple_to_dict(source_tuple=train_metrics, \n",
    "                                               target_dict=history_dict,\n",
    "                                               prefix_exclude=\"wrong_examples\",\n",
    "                                               prefix_to_add=\"test_\")\n",
    "        \n",
    "            test_loss = test_metrics.loss\n",
    "            min_test_loss = min(min_test_loss, test_loss)\n",
    "            \n",
    "            \n",
    "            imgs_rec = vae.forward(imgs_in=reference_imgs).imgs\n",
    "            tmp = show_batch(imgs_rec, n_padding=4, figsize=(12,12), title='epoch= {0:6d}'.format(epoch))\n",
    "            exp.log_image(\"imgs_rec\", tmp)\n",
    "                        \n",
    "            if((test_loss == min_test_loss) or ((epoch % CHECKPOINT_FREQUENCY) == 0)): \n",
    "                ckpt = create_ckpt(model=vae, \n",
    "                                   optimizer=optimizer, \n",
    "                                   history_dict=history_dict, \n",
    "                                   epoch=epoch, \n",
    "                                   hyperparams_dict=params)\n",
    "                \n",
    "                checkpoint_files += [os.path.join(dir_output, \"ckp_\"+str(epoch)+\".pkl\")]\n",
    "                history_files += [os.path.join(dir_output, \"history_\"+str(epoch)+\".pkl\")]\n",
    "            \n",
    "                save_obj(ckpt, checkpoint_files[-1])\n",
    "                save_dict_as_json(history_dict, history_file[-1])\n",
    "                \n",
    "                replace_artifact(experiment=exp, file_list=checkpoint_files)\n",
    "                replace_artifact(experiment=exp, file_list=history_files)\n",
    "                \n",
    "                print(\"saved files -> \"+checkpoint_files[-1]+\"  \"+history_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-84c77e9a3e6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'exp' is not defined"
     ]
    }
   ],
   "source": [
    "exp.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM OF ANTIALIASING\n",
    "READ MASK CNN TO SEE HOW THEY DO THE REGION PROPOSAL\n",
    "maybe the problem is to have two transaformation in sequence (first downscaling and then cropping)\n",
    "maybe the encoder should take the raw image not the outcome of the unet. Outcome of unet is good for mask probably.\n",
    "test what happends in big code if encoder takes the raw image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "neptune": {
   "notebookId": "0dd08974-9074-4e82-8b9d-31e595cbea7f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
