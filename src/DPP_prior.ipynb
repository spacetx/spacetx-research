{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "#from MODULES.utilities_ml import FiniteDPP, SimilarityKernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the function here to make this notebook self contained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from torch.distributions.distribution import Distribution\n",
    "from torch.distributions import constraints\n",
    "\n",
    "class SimilarityKernel(torch.nn.Module):\n",
    "    \"\"\" Similarity based on sum of gaussian kernels of different strength and length_scales \"\"\"\n",
    "    def __init__(self, n_kernels: int = 4,\n",
    "                 pbc: bool = False,\n",
    "                 eps: float = 1E-4,\n",
    "                 length_scales: Optional[torch.Tensor] = None,\n",
    "                 kernel_weights: Optional[torch.Tensor] = None):\n",
    "        \"\"\" It is safer to set pbc=False b/c the matrix might become ill-conditioned otherwise \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_kernels = n_kernels\n",
    "        self.eps = eps\n",
    "        self.pbc = pbc\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "        if length_scales is None:\n",
    "            LENGTH_2 = 10.0\n",
    "            length_scales = torch.linspace(LENGTH_2/self.n_kernels, LENGTH_2,\n",
    "                                           steps=self.n_kernels,\n",
    "                                           device=self.device,\n",
    "                                           dtype=torch.float)\n",
    "        else:\n",
    "            length_scales = self._invertsoftplus(length_scales.float().to(self.device))\n",
    "        assert length_scales.shape[0] == self.n_kernels\n",
    "        self.similarity_length = torch.nn.Parameter(data=length_scales, requires_grad=True)\n",
    "\n",
    "        if kernel_weights is None:\n",
    "            kernel_weights = torch.ones(self.n_kernels,\n",
    "                                        device=self.device,\n",
    "                                        dtype=torch.float)/self.n_kernels\n",
    "        else:\n",
    "            kernel_weights = self._invertsoftplus(kernel_weights.float().to(self.device))\n",
    "        assert kernel_weights.shape[0] == self.n_kernels\n",
    "        self.similarity_w = torch.nn.Parameter(data=kernel_weights, requires_grad=True)\n",
    "\n",
    "        # Initialization\n",
    "        self.n_width = -1\n",
    "        self.n_height = -1\n",
    "        self.d2 = None\n",
    "        self.diag = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _invertsoftplus(x):\n",
    "        return torch.log(torch.exp(x)-1.0)\n",
    "\n",
    "    def _compute_d2_diag(self, n_width: int, n_height: int):\n",
    "        with torch.no_grad():\n",
    "            ix_array = torch.arange(start=0, end=n_width, dtype=torch.int, device=self.device)\n",
    "            iy_array = torch.arange(start=0, end=n_height, dtype=torch.int, device=self.device)\n",
    "            ix_grid, iy_grid = torch.meshgrid([ix_array, iy_array])\n",
    "            map_points = torch.stack((ix_grid, iy_grid), dim=-1)  # n_width, n_height, 2\n",
    "            locations = map_points.flatten(start_dim=0, end_dim=-2)  # (n_width*n_height, 2)\n",
    "            d = (locations.unsqueeze(-2) - locations.unsqueeze(-3)).abs()  # (n_width*n_height, n_width*n_height, 2)\n",
    "            if self.pbc:\n",
    "                d_pbc = d.clone()\n",
    "                d_pbc[..., 0] = -d[..., 0] + n_width\n",
    "                d_pbc[..., 1] = -d[..., 1] + n_height\n",
    "                d2 = torch.min(d, d_pbc).pow(2).sum(dim=-1).float()\n",
    "            else:\n",
    "                d2 = d.pow(2).sum(dim=-1).float()\n",
    "\n",
    "            values = self.eps * torch.ones(d2.shape[-2], dtype=torch.float, device=self.device)\n",
    "            diag = torch.diag_embed(values, offset=0, dim1=-2, dim2=-1)\n",
    "            return d2, diag\n",
    "\n",
    "    def sample_2_mask(self, sample):\n",
    "        independent_dims = list(sample.shape[:-1])\n",
    "        mask = sample.view(independent_dims + [self.n_width, self.n_height])\n",
    "        return mask\n",
    "\n",
    "    def get_l_w(self):\n",
    "        return F.softplus(self.similarity_length)+1.0, F.softplus(self.similarity_w)+1E-2\n",
    "\n",
    "    def forward(self, n_width: int, n_height: int):\n",
    "        \"\"\" Implement L = sum_i a_i exp[-b_i d2] \"\"\"\n",
    "        l, w = self.get_l_w()\n",
    "        l2 = l.pow(2)\n",
    "\n",
    "        if (n_width != self.n_width) or (n_height != self.n_height):\n",
    "            self.n_width = n_width\n",
    "            self.n_height = n_height\n",
    "            self.d2, self.diag = self._compute_d2_diag(n_width=n_width, n_height=n_height)\n",
    "\n",
    "        likelihood_kernel = (w[..., None, None] *\n",
    "                             torch.exp(-0.5*self.d2/l2[..., None, None])).sum(dim=-3) + self.diag\n",
    "        return likelihood_kernel  # shape (n_width*n_height, n_width*n_height)\n",
    "\n",
    "class FiniteDPP(Distribution):\n",
    "    \"\"\" Finite DPP distribution defined via:\n",
    "        1. L = likelihood kernel of shape *,n,n\n",
    "        2. K = correlation kernel of shape *,n,n\n",
    "\n",
    "        The constraints are:\n",
    "        K = positive semidefinite, symmetric, eigenvalues in [0,1]\n",
    "        L = positive semidefinite, symmetric, eigenvalues >= 0\n",
    "\n",
    "        Need to be careful about svd decomposition which can become unstable on GPU or CPU\n",
    "        https://github.com/pytorch/pytorch/issues/28293\n",
    "    \"\"\"\n",
    "\n",
    "    arg_constraints = {'K': constraints.positive_definite,\n",
    "                       'L': constraints.positive_definite}\n",
    "    support = constraints.boolean\n",
    "    has_rsample = False\n",
    "\n",
    "    def __init__(self, K=None, L=None, validate_args=None):\n",
    "\n",
    "        if (K is None and L is None) or (K is not None and L is not None):\n",
    "            raise Exception(\"only one among K and L need to be defined\")\n",
    "\n",
    "        elif K is not None:\n",
    "            self.K = 0.5 * (K + K.transpose(-1, -2))  # make sure it is symmetrized\n",
    "            try:\n",
    "                u, s_k, v = torch.svd(self.K)\n",
    "            except:\n",
    "                # torch.svd may have convergence issues for GPU and CPU.\n",
    "                u, s_k, v = torch.svd(self.K + 1e-3 * self.K.mean() * torch.ones_like(self.K))\n",
    "            s_l = s_k / (1.0 - s_k)\n",
    "            self.L = torch.matmul(u * s_l.unsqueeze(-2), v.transpose(-1, -2))\n",
    "\n",
    "            # Debug block\n",
    "            # tmp = torch.matmul(u * s_k.unsqueeze(-2), v.transpose(-1, -2))\n",
    "            # check = (tmp - self.K).abs().max()\n",
    "            # print(\"check ->\",check)\n",
    "            # assert check < 1E-4\n",
    "\n",
    "        elif L is not None:\n",
    "            self.L = 0.5 * (L + L.transpose(-1, -2))  # make sure it is symmetrized\n",
    "            try:\n",
    "                u, s_l, v = torch.svd(self.L)\n",
    "            except:\n",
    "                # torch.svd may have convergence issues for GPU and CPU.\n",
    "                u, s_l, v = torch.svd(self.L + 1e-3 * self.L.mean() * torch.ones_like(self.L))\n",
    "            s_k = s_l / (1.0 + s_l)\n",
    "            self.K = torch.matmul(u * s_k.unsqueeze(-2), v.transpose(-1, -2))\n",
    "\n",
    "            # Debug block\n",
    "            # tmp = torch.matmul(u * s_l.unsqueeze(-2), v.transpose(-1, -2))\n",
    "            # check = (tmp - self.L).abs().max()\n",
    "            # print(\"check ->\",check)\n",
    "            # assert check < 1E-4\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "        self.s_l = s_l\n",
    "        batch_shape, event_shape = self.K.shape[:-2], self.K.shape[-1:]\n",
    "        super(FiniteDPP, self).__init__(batch_shape, event_shape, validate_args=validate_args)\n",
    "\n",
    "    def expand(self, batch_shape, _instance=None):\n",
    "        new = self._get_checked_instance(FiniteDPP, _instance)\n",
    "        batch_shape = torch.Size(batch_shape)\n",
    "        kernel_shape = batch_shape + self.event_shape + self.event_shape\n",
    "        value_shape = batch_shape + self.event_shape\n",
    "        new.s_l = self.s_l.expand(value_shape)\n",
    "        new.L = self.L.expand(kernel_shape)\n",
    "        new.K = self.K.expand(kernel_shape)\n",
    "        super(FiniteDPP, new).__init__(batch_shape,\n",
    "                                       self.event_shape,\n",
    "                                       validate_args=False)\n",
    "        new._validate_args = self._validate_args\n",
    "        return new\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        shape_value = self._extended_shape(sample_shape)  # shape = sample_shape + batch_shape + event_shape\n",
    "        shape_kernel = shape_value + self._event_shape  # shape = sample_shape + batch_shape + event_shape + event_shape\n",
    "\n",
    "        with torch.no_grad():\n",
    "            K = self.K.expand(shape_kernel).clone()\n",
    "            value = torch.zeros(shape_value, dtype=torch.bool, device=K.device)\n",
    "            rand = torch.rand(shape_value, dtype=K.dtype, device=K.device)\n",
    "\n",
    "            for j in range(rand.shape[-1]):\n",
    "                c = rand[..., j] < K[..., j, j]\n",
    "                value[..., j] = c\n",
    "                K[..., j, j] -= (~c).to(K.dtype)\n",
    "                K[..., j + 1:, j] /= K[..., j, j].unsqueeze(-1)\n",
    "                K[..., j + 1:, j + 1:] -= K[..., j + 1:, j].unsqueeze(-1) * K[..., j, j + 1:].unsqueeze(-2)\n",
    "\n",
    "            return value\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        \"\"\" log_prob = logdet(Ls) - logdet(L+I)\n",
    "            I am using the fact that eigen(L+I) = eigen(L)+1\n",
    "            -> logdet(L+I)=log prod[ eigen(L+I) ] = sum log(eigen(L+I)) = sum log(eigen(L)+1)\n",
    "\n",
    "            # value.shape = sample_shape + batch_shape + event_shape\n",
    "            # logdet(L+I).shape = batch_shape\n",
    "            :rtype:\n",
    "        \"\"\"\n",
    "        assert are_broadcastable(value, self.L[..., 0])\n",
    "        assert self.L.device == value.device\n",
    "        assert value.dtype == torch.bool\n",
    "\n",
    "        if self._validate_args:\n",
    "            self._validate_sample(value)\n",
    "\n",
    "        logdet_L_plus_I = (self.s_l + 1).log().sum(dim=-1)  # batch_shape\n",
    "\n",
    "        # Reshapes\n",
    "        independet_dims = list(value.shape[:-1])\n",
    "        value = value.flatten(start_dim=0, end_dim=-2)  # *, event_shape\n",
    "        L = self.L.expand(independet_dims + [-1, -1]).flatten(start_dim=0, end_dim=-3)  # *, event_shape, event_shape\n",
    "\n",
    "        n_max = torch.sum(value, dim=-1).max().item()\n",
    "        matrix = torch.eye(n_max, dtype=L.dtype, device=L.device).expand(L.shape[-3], n_max, n_max).clone()\n",
    "        for i in range(value.shape[0]):\n",
    "            n = torch.sum(value[i]).item()\n",
    "            matrix[i, :n, :n] = L[i, value[i], :][:, value[i]]\n",
    "        logdet_Ls = torch.logdet(matrix).view(independet_dims)  # sample_shape, batch_shape\n",
    "        return logdet_Ls - logdet_L_plus_I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make fig of DPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 20 0.1\n",
      "0 1 20 1\n",
      "0 2 20 5\n",
      "0 3 20 10\n",
      "1 0 10 0.1\n",
      "1 1 10 1\n",
      "1 2 10 5\n",
      "1 3 10 10\n",
      "2 0 5 0.1\n",
      "2 1 5 1\n",
      "2 2 5 5\n",
      "2 3 5 10\n",
      "3 0 1 0.1\n",
      "3 1 1 1\n",
      "3 2 1 5\n",
      "3 3 1 10\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax =  plt.subplots(ncols=4, nrows=4, figsize=(8,8))\n",
    "for r, length_scale in enumerate([20,10,5,1]):\n",
    "    for c, prefactor in enumerate([0.1, 1, 5, 10]):\n",
    "        print(r,c,length_scale,prefactor)\n",
    "        \n",
    "        similarity = SimilarityKernel(n_kernels=1, \n",
    "                                      length_scales=torch.tensor([length_scale]),\n",
    "                                      kernel_weights=torch.tensor([prefactor]))\n",
    "        L = similarity(n_width=20, n_height=20)\n",
    "        DPP = FiniteDPP(L=L)\n",
    "        value = DPP.sample(sample_shape=torch.Size([3]))  # draw three samples\n",
    "        mask = similarity.sample_2_mask(value)\n",
    "        ax[r,c].axis('off')\n",
    "        ax[r,c].imshow(mask[0], cmap='gray')\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"./DPP_samples.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "neptune": {
   "notebookId": "ea58cca6-5f9c-4037-8151-a3c4003e7b30"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
