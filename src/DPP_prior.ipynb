{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b,w,h = 2,20,20\n",
    "prob_corr_factor = 0.23\n",
    "n_objects_max = 20\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MODULES.utilities_ml import FiniteDPP, SimilarityKernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l,w -> (10.010000228881836, 0.11000004410743713)\n",
      "l,w -> (10.010000228881836, 1.0099999904632568)\n",
      "l,w -> (0.11000004410743713, 0.11000004410743713)\n",
      "l,w -> (0.11000004410743713, 1.0099999904632568)\n"
     ]
    }
   ],
   "source": [
    "list_of_output = []\n",
    "list_of_input = []\n",
    "w_list = [0.1, 1, 0.1, 1]\n",
    "l_list = [10, 10, 0.1, 0.1]\n",
    "for l,w in zip(l_list,w_list):\n",
    "\n",
    "    similarity = SimilarityKernel(n_kernels=1, \n",
    "                                  length_scales=l*torch.ones(1),\n",
    "                                  kernel_weights=w*torch.ones(1))\n",
    "    L = similarity(n_width=20, n_height=20)\n",
    "    DPP = FiniteDPP(L=L)\n",
    "    value = DPP.sample(sample_shape=torch.Size([3]))\n",
    "    mask = similarity.sample_2_mask(value)\n",
    "\n",
    "    my_input = (similarity.get_l_w()[0].item(), similarity.get_l_w()[1].item())\n",
    "    print(\"l,w ->\",my_input)\n",
    "    list_of_input.append(my_input)\n",
    "    list_of_output.append(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.11000004410743713, 1.0099999904632568)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAACQCAYAAAAFpRFcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAFHklEQVR4nO3dwY6bQBAEUIjy/7/snPYW0LrdHprivWtiAzMmKo1U6f31em0AAEn+XH0DAADdBBwAII6AAwDEEXAAgDgCDgAQ5+/ZH+77rmLFtm3b9nq99t/8Pb8Zfvz2N7Nt9/3dHLVQ9/3Xjx5xD506/q2prMnVjeLKvVWf5+hzlc+cWbgP//1CJzgAQBwBBwCII+AAAHEEHAAgjoADAMQRcACAOKc1cQCOdVaxqxXdq+vg3XXkb6pct3t9Vz37hEp85/5XnscJDgAQR8ABAOIIOABAHAEHAIgj4AAAcbSoWCZtKCB0uut7ULnvbzd8Vg2TrLjjvW3busGZnc/qBAcAiCPgAABxBBwAII6AAwDEEXAAgDgCDgAQR02cZe5ag11Fjf77VtVWJ1R+K1bd91XDNs90vn/dzzdxvT6xak2d4AAAcQQcACCOgAMAxBFwAIA4Ag4AEEeL6gN3bUowk9/M900YWjjZhPX5bVtmQiOu0/S2XudQ1VVr6gQHAIgj4AAAcQQcACCOgAMAxBFwAIA4Ag4AEEdN/AN3rYICJKvUk48+U6m0d9TgP/1MxYShnp3Vcic4AEAcAQcAiCPgAABxBBwAII6AAwDE0aICKFrVopnQ2Kw867vf1aVzXyYMweze/85n7bz+mcreOcEBAOIIOABAHAEHAIgj4AAAcQQcACCOgAMAxFETh+362uTTdddqV5lQ+V2l8747avQT6+k/7rrHE97Dzj1yggMAxBFwAIA4Ag4AEEfAAQDiCDgAQBwtKtju23pIYf2Zqrt5tWpAa7erB8tWvssJDgAQR8ABAOIIOABAHAEHAIgj4AAAcQQcACCOmjhA0aohrROGIB6ZeG9pQzUr1ekJwzGvHtDqBAcAiCPgAABxBBwAII6AAwDEEXAAgDhaVLCta8OsMqFdkWLCWk7es8n39o7Kc6T9u3Gm81kr75RhmwAAm4ADAAQScACAOAIOABBHwAEA4gg4AECc9pr4hEolvCvtt7mquglXqdSGu9+LymeuHkBZ/b6rr1PhBAcAiCPgAABxBBwAII6AAwDEEXAAgDgCDgAQp70mrk4K9+Td/T/rMlOlIl2pb6/a/1UTtquuXofKfjvBAQDiCDgAQBwBBwCII+AAAHEEHAAgTnuLCqiptAfgqb+bycNhJ7e1qjobW2fP2rkOTnAAgDgCDgAQR8ABAOIIOABAHAEHAIgj4AAAcdTEeYzJtdIp93CV6Xsz2VPXpzJ88a6fOVK5zpmVwztX3IMTHAAgjoADAMQRcACAOAIOABBHwAEA4mhR8RjdbZOnDjn8BntzLq1l1tGU6W78HK1j99p3XmfVGnRfa9WzOsEBAOIIOABAHAEHAIgj4AAAcQQcACCOgAMAxFETh6I71nOfrlI1rezzqmrxXa2qI7/z3Z1rXLnOyj2+unbe/R4efZ8THAAgjoADAMQRcACAOAIOABBHwAEA4kS2qNIG7cG70oYzvuvqZ1w1PPQb17qL7iZWZ4Oo+966m3ffbKp9qvPenOAAAHEEHAAgjoADAMQRcACAOAIOABBHwAEA4kTWxJ9am4Qf3oEs9vM9lSGPnYMhVw2TrN7DqvVZ9V+2GLYJADyGgAMAxBFwAIA4Ag4AEEfAAQDiRLaomMkQ1H6rBvdBgglDKzvbWhOGelasujcnOABAHAEHAIgj4AAAcQQcACCOgAMAxBFwAIA4e6WuBQAwmRMcACCOgAMAxBFwAIA4Ag4AEEfAAQDiCDgAQJx/1HrTnjNL7G4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(list_of_input[-1])\n",
    "\n",
    "print()\n",
    "mask = list_of_output[-1]\n",
    "\n",
    "fig, ax =  plt.subplots(ncols=4, figsize=(8,8))\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "ax[2].axis('off')\n",
    "ax[3].axis('off')\n",
    "ax[0].imshow(list_of_output[0][0], cmap='gray')\n",
    "ax[1].imshow(list_of_output[1][0], cmap='gray')\n",
    "ax[2].imshow(list_of_output[2][0], cmap='gray')\n",
    "ax[3].imshow(list_of_output[3][0], cmap='gray')\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"./DPP_samples.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity.get_sigma2_w()[0].item()\n",
    "similarity.get_sigma2_w()[1].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_mask[(tensor([1.]),\n",
    "  tensor([10.]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(similarity.similarity_s2)\n",
    "print(similarity.get_sigma2_w())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(log_p)\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "\n",
    "fig, ax =  plt.subplots(ncols=2, nrows=2, figsize=(8,8))\n",
    "ax[0,0].axis('off')\n",
    "ax[0,1].axis('off')\n",
    "ax[1,0].axis('off')\n",
    "ax[1,1].axis('off')\n",
    "ax[0,0].imshow(mask[0], cmap='gray')\n",
    "ax[0,1].imshow(mask[1], cmap='gray')\n",
    "ax[1,0].imshow(mask[2], cmap='gray')\n",
    "ax[1,1].imshow(mask.sum(dim=(-3)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_log_p = DPP.OLD_log_prob(value)\n",
    "print(OLD_log_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p = DPP.log_prob(value)\n",
    "print(log_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(similarity.similarity_s2.grad)\n",
    "print(similarity.similarity_w.grad)\n",
    "log_p.sum().backward()\n",
    "print(similarity.similarity_s2.grad)\n",
    "print(similarity.similarity_w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GUIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassBernoulli(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, p):\n",
    "        c = torch.rand_like(p)<p\n",
    "        return c\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "def pass_bernoulli(prob):\n",
    "    return PassBernoulli.apply(prob)\n",
    "    \n",
    "class PassMask(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, c, mask):\n",
    "        return c*nms_mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output\n",
    "    \n",
    "def pass_mask(c, mask):\n",
    "    return PassMask.apply(c, mask)\n",
    "\n",
    "def compute_nms(prob, c):\n",
    "    raise NotImplementedError\n",
    "    nms_mask = (c == 1)\n",
    "    return nms_mask\n",
    "\n",
    "\n",
    "raw =torch.randn((b,1,w,h))\n",
    "q = torch.sigmoid(raw)\n",
    "\n",
    "# Correction factor\n",
    "if prob_corr_factor == 0:\n",
    "    log_q = F.logsigmoid(raw)\n",
    "    log_one_minus_q = F.logsigmoid(-raw)\n",
    "else:\n",
    "    correction = torch.rand_like(q)\n",
    "    q = ((1-prob_corr_factor)*q + prob_corr_factor*correction).clamp(min=1E-4, max=1-1E-4)\n",
    "    log_q = torch.log(q)\n",
    "    log_one_minus_q = torch.log1p(-q)\n",
    "\n",
    "# sample, NMS, log_prob\n",
    "c = pass_bernoulli(prob=q)\n",
    "with torch.no_grad():\n",
    "    nms_mask = compute_nms(prob=q, c=c)  # only if c=1, there is NMS.\n",
    "c_mask = pass_mask(c, nms_mask) \n",
    "log_prob_posterior = (c_mask*log_q + ~c_mask*log_one_minus_q).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =  plt.subplots(ncols=3, figsize=(12,12))\n",
    "ax[0].imshow(q[0,0])\n",
    "ax[1].imshow(c[0,0])\n",
    "ax[2].imshow(c_mask[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.distribution import Distribution\n",
    "from torch.distributions import constraints\n",
    "from MODULES.utilities_ml import are_broadcastable\n",
    "\n",
    "class Similarity(torch.nn.Module):\n",
    "    \"\"\" Similarity based on sum of gaussian kernels of different strength and length_scales \"\"\"\n",
    "\n",
    "    def __init__(self, n_kernels: int = 4, eps: float = 1E-4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_kernels = n_kernels\n",
    "        self.eps = eps\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        \n",
    "        self.w = torch.nn.Parameter(data=torch.randn(self.n_kernels, \n",
    "                                                     device=self.device, \n",
    "                                                     dtype=torch.float), requires_grad=True)\n",
    "        self.b = torch.nn.Parameter(data=25*torch.ones(self.n_kernels, \n",
    "                                                     device=self.device, \n",
    "                                                     dtype=torch.float), requires_grad=True)\n",
    "        \n",
    "        # Initialization\n",
    "        self.n_width = -1\n",
    "        self.n_height = -1\n",
    "        self.d2 = None\n",
    "        self.diag = None \n",
    "        \n",
    "    def _compute_d2_diag(self, n_width: int, n_height: int):\n",
    "        with torch.no_grad():\n",
    "            ix_array = torch.arange(start=0, end=n_width, dtype=torch.int, device=self.device)\n",
    "            iy_array = torch.arange(start=0, end=n_height, dtype=torch.int, device=self.device)\n",
    "            ix_grid, iy_grid = torch.meshgrid([ix_array, iy_array])\n",
    "            map_points = torch.stack((ix_grid, iy_grid), dim=-1)  # n_width, n_height, 2\n",
    "            locations = map_points.flatten(start_dim=0, end_dim=-2)  # (n_width*n_height, 2)\n",
    "            d2 = (locations.unsqueeze(-2) - locations.unsqueeze(-3)).pow(2).sum(dim=-1).float()\n",
    "            diag = torch.eye(d2.shape[-2], \n",
    "                             dtype=torch.float, \n",
    "                             device=self.device, \n",
    "                             requires_grad=False) * self.eps\n",
    "            return d2, diag\n",
    "        \n",
    "    def sample_2_mask(self, sample):\n",
    "        independent_dims = list(sample.shape[:-1])\n",
    "        mask = sample.view(independent_dims+[self.n_width, self.n_height])\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, n_width: int, n_height: int):\n",
    "        \"\"\" Implement L = sum_i a_i exp[-b_i d2] \"\"\"\n",
    "        sigma2 = F.softplus(self.b).view(-1,1,1)  # add singleton for w,h\n",
    "        print(sigma2)\n",
    "        w = F.softplus(self.w).view(-1,1,1)  # add singleton for w,h\n",
    "        \n",
    "        if (n_width != self.n_width) or (n_height != self.n_height):\n",
    "            self.n_width=n_width\n",
    "            self.n_height=n_height\n",
    "            self.d2, self.diag = self._compute_d2_diag(n_width=n_width, n_height=n_height)\n",
    "                    \n",
    "        likelihood_kernel = (w*torch.exp(-0.5*self.d2/sigma2)).sum(dim=-3) + self.diag\n",
    "        return likelihood_kernel  # shape (n_width*n_height, n_width*n_height)\n",
    "    \n",
    "class FiniteDPP(Distribution):\n",
    "    \"\"\" Finite DPP distribution defined via:\n",
    "        1. L = likelihood kernel of shape *,n,n \n",
    "        2. K = correlation kernel of shape *,n,n \n",
    "        \n",
    "        The constraints are:\n",
    "        K = positive semidefinite, symmetric, eigenvalues in [0,1]\n",
    "        L = positive semidefinite, symmetric, eigenvalues >= 0\n",
    "    \"\"\"\n",
    "    \n",
    "    arg_constraints = {'K': constraints.positive_definite, \n",
    "                       'L': constraints.positive_definite}\n",
    "    support = constraints.boolean\n",
    "    has_rsample = False\n",
    "    \n",
    "    def __init__(self, K=None, L=None, validate_args=None):\n",
    "        \n",
    "        if (K is None and L is None) or (K is not None and L is not None):\n",
    "            raise Exception(\"only one among K and L need to be defined\")\n",
    "                    \n",
    "        elif K is not None:\n",
    "            self.K = 0.5*(K+K.transpose(-1,-2))  # make sure it is symmetrized\n",
    "            u,s_k,v = torch.svd(self.K)\n",
    "            s_l = s_k / (1.0-s_k)\n",
    "            self.L = torch.matmul(u * s_l.unsqueeze(-2), v.transpose(-1,-2))\n",
    "            \n",
    "            tmp = torch.matmul(u * s_k.unsqueeze(-2), v.transpose(-1,-2))\n",
    "            check = (tmp-self.K).abs().max()\n",
    "            # print(\"check ->\",check)\n",
    "            assert check < 1E-4\n",
    "            \n",
    "        else:\n",
    "            self.L = 0.5*(L+L.transpose(-1,-2))  # make sure it is symmetrized \n",
    "            u,s_l,v = torch.svd(self.L) \n",
    "            s_k = s_l / (1.0+s_l) \n",
    "            self.K = torch.matmul(u * s_k.unsqueeze(-2), v.transpose(-1,-2))\n",
    "            \n",
    "            tmp = torch.matmul(u * s_l.unsqueeze(-2), v.transpose(-1,-2))\n",
    "            check = (tmp-self.L).abs().max()\n",
    "            # print(\"check ->\",check)\n",
    "            assert check < 1E-4\n",
    "        \n",
    "        self.s_l = s_l            \n",
    "        batch_shape, event_shape = self.K.shape[:-2], self.K.shape[-1:]\n",
    "        super(FiniteDPP, self).__init__(batch_shape, event_shape, validate_args=validate_args)\n",
    "        \n",
    "    def expand(self, batch_shape, _instance=None):\n",
    "        new = self._get_checked_instance(FiniteDPP, _instance)\n",
    "        batch_shape = torch.Size(batch_shape)\n",
    "        kernel_shape = batch_shape + self.event_shape + self.event_shape\n",
    "        value_shape = batch_shape + self.event_shape \n",
    "        new.s_l = self.s_l.expand(value_shape) \n",
    "        new.L = self.L.expand(kernel_shape)\n",
    "        new.K = self.K.expand(kernel_shape)\n",
    "        super(FiniteDPP, new).__init__(batch_shape,\n",
    "                                       self.event_shape,\n",
    "                                       validate_args=False)\n",
    "        new._validate_args = self._validate_args\n",
    "        return new\n",
    "\n",
    "    \n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        shape_value = self._extended_shape(sample_shape)  # shape = sample_shape + batch_shape + event_shape\n",
    "        shape_kernel = shape_value + self._event_shape  # shape = sample_shape + batch_shape + event_shape + event_shape\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            K = self.K.expand(shape_kernel).clone()\n",
    "            value = torch.zeros(shape_value, dtype=torch.bool, device=K.device)\n",
    "            rand = torch.rand(shape_value, dtype=K.dtype, device=K.device)\n",
    "\n",
    "            for j in range(rand.shape[-1]):\n",
    "                c = rand[...,j]<K[...,j,j] \n",
    "                value[...,j] = c\n",
    "                K[..., j, j] -= (~c).to(K.dtype)\n",
    "                K[..., j + 1:, j] /= K[..., j, j].unsqueeze(-1)\n",
    "                K[..., j + 1:, j + 1:] -= K[..., j + 1:, j].unsqueeze(-1) * K[..., j, j + 1:].unsqueeze(-2)\n",
    "        \n",
    "            return value\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        \"\"\" log_prob = logdet(Ls) - logdet(L+I)\n",
    "            I am using the fact that eigen(L+I) = eigen(L)+1 \n",
    "            -> logdet(L+I)=log prod[ eigen(L+I) ] = sum log(eigen(L+I)) = sum log(eigen(L)+1) \n",
    "            \n",
    "            # value.shape = sample_shape + batch_shape + event_shape\n",
    "            # logdet(L+I).shape = batch_shape\n",
    "        \"\"\"\n",
    "        assert are_broadcastable(value, self.L[...,0])\n",
    "        assert self.L.device == value.device\n",
    "        assert value.dtype == torch.bool\n",
    "        \n",
    "        if self._validate_args:\n",
    "            self._validate_sample(value)\n",
    "        \n",
    "        logdet_L_plus_I = (self.s_l+1).log().sum(dim=-1)  # batch_shape\n",
    "        \n",
    "        # Reshapes\n",
    "        independet_dims = list(value.shape[:-1])\n",
    "        value = value.flatten(start_dim=0, end_dim=-2)  # *, event_shape\n",
    "        L = self.L.expand(independet_dims+[-1,-1]).flatten(start_dim=0, end_dim=-3)  # *, event_shape, event_shape\n",
    "        logdet_Ls = torch.zeros(independet_dims, dtype=self.L.dtype, device=value.device).view(-1)  # *\n",
    "        \n",
    "        # Select rows and columns of the matrix which correspond to selected particles\n",
    "        for i in range(logdet_Ls.shape[0]):\n",
    "            tmp = L[i,value[i],:][:,value[i]]\n",
    "            logdet_Ls[i] = torch.logdet(tmp)\n",
    "        logdet_Ls = logdet_Ls.view(independet_dims)  # sample_shape, batch_shape\n",
    "        return logdet_Ls - logdet_L_plus_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = Similarity()\n",
    "L = similarity(n_width=20, n_height=20)\n",
    "DPP = FiniteDPP(L=L)\n",
    "value = DPP.sample(sample_shape=torch.Size([3]))\n",
    "log_p = DPP.log_prob(value)\n",
    "mask = similarity.sample_2_mask(value)\n",
    "\n",
    "print(log_p)\n",
    "fig, ax =  plt.subplots(ncols=4, figsize=(12,12))\n",
    "ax[0].imshow(mask[0])\n",
    "ax[1].imshow(mask[1])\n",
    "ax[2].imshow(mask[2])\n",
    "ax[3].imshow(mask.sum(dim=(-3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
